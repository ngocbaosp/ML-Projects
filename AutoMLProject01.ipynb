{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoMLProject01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngocbaosp/ML-Projects/blob/master/AutoMLProject01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StxbZGOO2y3I",
        "colab_type": "text"
      },
      "source": [
        "# Install AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BisgUVyh-3x",
        "colab_type": "code",
        "outputId": "327fcbc9-40fc-4f14-d1e8-4bf7a162040c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,288 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 131331 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Collecting auto-sklearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/99/27caac4f6804be48722158e31c630e0737110581774df0615a36b21239aa/auto-sklearn-0.5.2.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.12)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Collecting scikit-learn<0.20,>=0.19 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Collecting lockfile (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Collecting liac-arff (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Collecting ConfigSpace<0.5,>=0.4.0 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/de/4e8e4f26332fc65404f52baa112defbf822b6738b60bfa6b2993f5c60933/ConfigSpace-0.4.10.tar.gz (882kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.0MB/s \n",
            "\u001b[?25hCollecting pynisher>=0.4.2 (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/21/c86c64c305da6d43fb89780d33cbc839c07736b71955a8bdb642a02b7538/pynisher-0.5.0.tar.gz\n",
            "Collecting pyrfr<0.8,>=0.7 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/c6/c555cfa3c7d0078dded091d4901ed52344bbb925077aa70b871faf35fd58/pyrfr-0.7.4.tar.gz (291kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 42.5MB/s \n",
            "\u001b[?25hCollecting smac==0.8 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ab/2b0a6fb00bd76e2415a04dcca453ad0b0db9b4218b02401306ff2bc6135d/smac-0.8.0.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.7.4)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Collecting sphinx_rtd_theme (from smac==0.8->auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/4df37087a1d36755e3a3bfd2a30263f358d2dea21938240fa02313d45f51/sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4MB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.7.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Building wheels for collected packages: auto-sklearn, liac-arff, ConfigSpace, pynisher, pyrfr, smac\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/21/43/182fed664b6474f88600c110c4ebd254d6256ba59175cef3fd\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
            "  Building wheel for ConfigSpace (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/83/cb/28dd42bac69c8867d485138030daa83841c7f84afe68b2fdf7\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2a/c4/ec3abc8a2f786ef9786ea8fe6ff629a4e54812a3f98cc41b47\n",
            "  Building wheel for pyrfr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/98/fd/b1d53cab6d5ed836980777d9733d7e549d82a727650eed6f6d\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/52/83/d2d66a840968025d072ddb1cd776fdc5eb5e337e1cc887bc3f\n",
            "Successfully built auto-sklearn liac-arff ConfigSpace pynisher pyrfr smac\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, lockfile, liac-arff, ConfigSpace, pynisher, pyrfr, sphinx-rtd-theme, smac, auto-sklearn\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "Successfully installed ConfigSpace-0.4.10 auto-sklearn-0.5.2 liac-arff-2.4.0 lockfile-0.12.2 pynisher-0.5.0 pyrfr-0.7.4 scikit-learn-0.19.2 smac-0.8.0 sphinx-rtd-theme-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Z7JRDni3Ms",
        "colab_type": "code",
        "outputId": "d7a609d3-4e7f-4e6a-d8ac-8e98fead1b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install auto-sklearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.2)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.19.2)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.4.10)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: pyrfr<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.7.4)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.12)\n",
            "Requirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Requirement already satisfied: smac==0.8 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.8.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.7.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (0.4.3)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.7.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9o5GpGi7e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "\n",
        "# Load data\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3zsYPyooEVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE4j6toFjO",
        "colab_type": "code",
        "outputId": "4d7123e2-39fb-486b-cca2-1e91328e031f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import autosklearn.classification\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVxpdnVXjA-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d73769c2-bdd1-4761-de18-56be93875412"
      },
      "source": [
        "\n",
        "# configure auto-sklearn\n",
        "automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "          time_left_for_this_task=120, # run auto-sklearn for at most 2min\n",
        "          per_run_time_limit=30, # spend at most 30 sec for each model training\n",
        "          )\n",
        "\n",
        "# train model(s)\n",
        "automl.fit(X_train, y_train)\n",
        "\n",
        "# evaluate\n",
        "y_hat = automl.predict(X_test)\n",
        "test_acc = sklearn.metrics.accuracy_score(y_test, y_hat)\n",
        "print(\"Test Accuracy score {0}\".format(test_acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 19:46:10,678:EnsembleBuilder(1):d74860caaa557f473ce23908ff7ba369] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 19:46:10,696:EnsembleBuilder(1):d74860caaa557f473ce23908ff7ba369] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "Process pynisher function call:\n",
            "Process pynisher function call:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/__init__.py\", line 30, in fit_predict_try_except_decorator\n",
            "    return ta(queue=queue, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/ensemble_builder.py\", line 234, in main\n",
            "    time.sleep(self.sleep_duration)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 648, in eval_holdout\n",
            "    evaluator.fit_predict_and_loss(iterative=iterative)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 160, in fit_predict_and_loss\n",
            "    i, train_indices=train_split, test_indices=test_split\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 406, in _partial_fit_and_predict\n",
            "    self.Y_train[train_indices])\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/abstract_evaluator.py\", line 481, in _fit_and_suppress_warnings\n",
            "    model.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 93, in fit\n",
            "    self.fit_estimator(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 110, in fit_estimator\n",
            "    self._final_estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 402, in fit\n",
            "    return self.choice.fit(X, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 163, in fit\n",
            "    self.iterative_fit(X, y, n_iter=n_iter, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/classification/gradient_boosting.py\", line 90, in iterative_fit\n",
            "    self.estimator.fit(X, y, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1034, in fit\n",
            "    begin_at_stage, monitor, X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1089, in _fit_stages\n",
            "    X_csc, X_csr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 798, in _fit_stage\n",
            "    self.learning_rate, k=k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 249, in update_terminal_regions\n",
            "    y_pred[:, k], sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 572, in _update_terminal_region\n",
            "    numerator = np.sum(sample_weight * residual)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 2076, in sum\n",
            "    initial=initial)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\n",
            "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "KeyboardInterrupt\n",
            "Process pynisher function call:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/__init__.py\", line 30, in fit_predict_try_except_decorator\n",
            "    return ta(queue=queue, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 648, in eval_holdout\n",
            "    evaluator.fit_predict_and_loss(iterative=iterative)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 160, in fit_predict_and_loss\n",
            "    i, train_indices=train_split, test_indices=test_split\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 406, in _partial_fit_and_predict\n",
            "    self.Y_train[train_indices])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/abstract_evaluator.py\", line 481, in _fit_and_suppress_warnings\n",
            "    model.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 93, in fit\n",
            "    self.fit_estimator(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 110, in fit_estimator\n",
            "    self._final_estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 402, in fit\n",
            "    return self.choice.fit(X, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 163, in fit\n",
            "    self.iterative_fit(X, y, n_iter=n_iter, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/classification/gradient_boosting.py\", line 90, in iterative_fit\n",
            "    self.estimator.fit(X, y, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1034, in fit\n",
            "    begin_at_stage, monitor, X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1089, in _fit_stages\n",
            "    X_csc, X_csr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 788, in _fit_stage\n",
            "    check_input=False, X_idx_sorted=X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\", line 1124, in fit\n",
            "    X_idx_sorted=X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\", line 362, in fit\n",
            "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
            "KeyboardInterrupt\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000003.ensemble']\n",
            "Test Accuracy score 0.9933333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X89LTFhoI2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.sprint_statistics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukJU3_roQjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.show_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmixqkcMoav8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.cv_results_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GjDTVskomp1",
        "colab_type": "text"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eWSIgFWo8O2",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a59x2kHcpQv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXK2-zGSpCxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% md\n",
        "#### MyPCA\n",
        "#%%\n",
        "def myPCA(data,n):\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(data)\n",
        "    df = pca.transform(data)\n",
        "    PCA_Data = pd.DataFrame(df)\n",
        "    return PCA_Data\n",
        "\n",
        "#%% md\n",
        "#### myNormalize\n",
        "#%%\n",
        "def myNormalize(data):\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    Normalized_Data = min_max_scaler.fit_transform(data)\n",
        "    Normalized_Data = pd.DataFrame(Normalized_Data)\n",
        "    return Normalized_Data\n",
        "\n",
        "#%% md\n",
        "#### myEncode\n",
        "#%%\n",
        "def myEncode(data,col): \n",
        "    NewData_Encode = data.copy()\n",
        "    NewData_Encode = pd.get_dummies(NewData_Encode, columns=col, prefix = col)\n",
        "    return NewData_Encode\n",
        "\n",
        "\n",
        "#%% md\n",
        "#### myCleanAndTransformData\n",
        "#%%\n",
        "def myCleanAndTransformData(data):\n",
        "    \n",
        "    #Drop null rows\n",
        "    NewData = data.dropna()\n",
        "    #Remove unknown ata\n",
        "    NewData = NewData[NewData['episodes']!='Unknown']\n",
        "    #Add a new column rating class \n",
        "    NewData['Class']=1\n",
        "    # 1: High\n",
        "    # or 0: Low based on rating\n",
        "    NewData.loc[NewData['rating'] >= NewData['rating'].mean(), 'Class'] = 1\n",
        "    NewData.loc[NewData['rating'] < NewData['rating'].mean(), 'Class'] = 0\n",
        "    \n",
        "    #Split genre values into rows\n",
        "    NewData = pd.DataFrame(NewData.genre.str.split(',').tolist(), index=[NewData.anime_id,NewData.type,NewData.episodes,NewData.rating,NewData.members,NewData.Class]).stack()\n",
        "    NewData = NewData.reset_index([0,'anime_id','type','episodes','rating','members','Class'])\n",
        "    NewData.columns=['anime_id','type','episodes','rating','members','Class','genre']\n",
        "    \n",
        "    #Encode type feature: 6 unique values\n",
        "    NewData = myEncode(NewData,['type'])\n",
        " \n",
        "    #Encode genre feature: 82 unique values\n",
        "    NewData = myEncode(NewData,['genre'])\n",
        " \n",
        "     #Drop anmie_id,rating,Class\n",
        "    NewData = NewData.drop(['rating'],axis=1)\n",
        "    NewData = NewData.drop(columns=['anime_id'])\n",
        "    NewData = NewData.drop(columns=['episodes'])  \n",
        "    \n",
        "    return NewData\n",
        "\n",
        "\n",
        "#%% md\n",
        "#### mySplitData\n",
        "#%%\n",
        "def mySplitData(X_Data,Y_Data,test_size,random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def mySplitDataByTrainSize(X_Data,Y_Data,train_size,random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, train_size=train_size, random_state=random_state)\n",
        "    X_train, X_test, y_train, y_test = mySplitData(X_train,y_train,0.33,random_state)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWwslp1EpaUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bbc6631e-d9e7-4cc5-b5ea-996955e41ad2"
      },
      "source": [
        "#%% md\n",
        "# Load data from files\n",
        "#%%\n",
        "RawData = pd.read_csv('anime.csv')\n",
        "RawData.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anime_id</th>\n",
              "      <th>name</th>\n",
              "      <th>genre</th>\n",
              "      <th>type</th>\n",
              "      <th>episodes</th>\n",
              "      <th>rating</th>\n",
              "      <th>members</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32281</td>\n",
              "      <td>Kimi no Na wa.</td>\n",
              "      <td>Drama, Romance, School, Supernatural</td>\n",
              "      <td>Movie</td>\n",
              "      <td>1</td>\n",
              "      <td>9.37</td>\n",
              "      <td>200630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5114</td>\n",
              "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
              "      <td>Action, Adventure, Drama, Fantasy, Magic, Mili...</td>\n",
              "      <td>TV</td>\n",
              "      <td>64</td>\n",
              "      <td>9.26</td>\n",
              "      <td>793665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28977</td>\n",
              "      <td>Gintama°</td>\n",
              "      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n",
              "      <td>TV</td>\n",
              "      <td>51</td>\n",
              "      <td>9.25</td>\n",
              "      <td>114262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9253</td>\n",
              "      <td>Steins;Gate</td>\n",
              "      <td>Sci-Fi, Thriller</td>\n",
              "      <td>TV</td>\n",
              "      <td>24</td>\n",
              "      <td>9.17</td>\n",
              "      <td>673572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9969</td>\n",
              "      <td>Gintama&amp;#039;</td>\n",
              "      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n",
              "      <td>TV</td>\n",
              "      <td>51</td>\n",
              "      <td>9.16</td>\n",
              "      <td>151266</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   anime_id                              name  ... rating members\n",
              "0     32281                    Kimi no Na wa.  ...   9.37  200630\n",
              "1      5114  Fullmetal Alchemist: Brotherhood  ...   9.26  793665\n",
              "2     28977                          Gintama°  ...   9.25  114262\n",
              "3      9253                       Steins;Gate  ...   9.17  673572\n",
              "4      9969                     Gintama&#039;  ...   9.16  151266\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwC0YFqhptN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b28792a5-aa2c-47ec-ea4a-9a0b24073f87"
      },
      "source": [
        "#%% md\n",
        "#### Clean and Transform Data\n",
        "#%%\n",
        "Cleaned_Data = myCleanAndTransformData(RawData)\n",
        "Y_Data = Cleaned_Data['Class']\n",
        "X_Data = Cleaned_Data.drop(columns=['Class'])\n",
        "\n",
        "#%% md\n",
        "#### Normalize  Data\n",
        "#%%\n",
        "Normalized_Data = myNormalize(X_Data)\n",
        "#%% md\n",
        "#### PCA\n",
        "#%%\n",
        "n_components=40\n",
        "PCA_Data = myPCA(Normalized_Data,n_components)\n",
        "PCA_Data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.311391</td>\n",
              "      <td>0.786561</td>\n",
              "      <td>-0.420790</td>\n",
              "      <td>0.005234</td>\n",
              "      <td>-0.078663</td>\n",
              "      <td>-0.049646</td>\n",
              "      <td>-0.062640</td>\n",
              "      <td>0.007159</td>\n",
              "      <td>-0.075350</td>\n",
              "      <td>-0.030938</td>\n",
              "      <td>0.086247</td>\n",
              "      <td>-0.139425</td>\n",
              "      <td>-0.157022</td>\n",
              "      <td>0.028365</td>\n",
              "      <td>-0.081108</td>\n",
              "      <td>-0.232703</td>\n",
              "      <td>-0.299120</td>\n",
              "      <td>0.804699</td>\n",
              "      <td>-0.258797</td>\n",
              "      <td>-0.007687</td>\n",
              "      <td>-0.094831</td>\n",
              "      <td>-0.108054</td>\n",
              "      <td>-0.062475</td>\n",
              "      <td>0.025711</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>-0.024983</td>\n",
              "      <td>-0.033523</td>\n",
              "      <td>-0.004934</td>\n",
              "      <td>-0.011702</td>\n",
              "      <td>-0.006729</td>\n",
              "      <td>-0.011742</td>\n",
              "      <td>-0.014507</td>\n",
              "      <td>0.009433</td>\n",
              "      <td>-0.010485</td>\n",
              "      <td>-0.008221</td>\n",
              "      <td>-0.004127</td>\n",
              "      <td>0.005306</td>\n",
              "      <td>-0.012853</td>\n",
              "      <td>-0.006433</td>\n",
              "      <td>-0.008562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.284662</td>\n",
              "      <td>0.764035</td>\n",
              "      <td>-0.411982</td>\n",
              "      <td>-0.010872</td>\n",
              "      <td>-0.110067</td>\n",
              "      <td>-0.087035</td>\n",
              "      <td>-0.096786</td>\n",
              "      <td>0.054585</td>\n",
              "      <td>-0.179466</td>\n",
              "      <td>-0.045549</td>\n",
              "      <td>0.764383</td>\n",
              "      <td>0.581495</td>\n",
              "      <td>0.033808</td>\n",
              "      <td>-0.066725</td>\n",
              "      <td>0.030970</td>\n",
              "      <td>0.068166</td>\n",
              "      <td>0.010372</td>\n",
              "      <td>-0.031955</td>\n",
              "      <td>-0.043632</td>\n",
              "      <td>0.008161</td>\n",
              "      <td>-0.027471</td>\n",
              "      <td>-0.040192</td>\n",
              "      <td>-0.033360</td>\n",
              "      <td>0.004737</td>\n",
              "      <td>0.006610</td>\n",
              "      <td>-0.016134</td>\n",
              "      <td>-0.028708</td>\n",
              "      <td>-0.009434</td>\n",
              "      <td>-0.005291</td>\n",
              "      <td>-0.004034</td>\n",
              "      <td>-0.007216</td>\n",
              "      <td>-0.013007</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>-0.011350</td>\n",
              "      <td>-0.008301</td>\n",
              "      <td>-0.001372</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>-0.012475</td>\n",
              "      <td>-0.008168</td>\n",
              "      <td>-0.009003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.284659</td>\n",
              "      <td>0.767955</td>\n",
              "      <td>-0.395541</td>\n",
              "      <td>-0.007615</td>\n",
              "      <td>-0.091869</td>\n",
              "      <td>-0.059768</td>\n",
              "      <td>-0.062093</td>\n",
              "      <td>0.036484</td>\n",
              "      <td>-0.086830</td>\n",
              "      <td>-0.024722</td>\n",
              "      <td>0.092133</td>\n",
              "      <td>-0.282600</td>\n",
              "      <td>-0.452072</td>\n",
              "      <td>-0.568979</td>\n",
              "      <td>0.527521</td>\n",
              "      <td>0.286982</td>\n",
              "      <td>0.035407</td>\n",
              "      <td>-0.060924</td>\n",
              "      <td>-0.105755</td>\n",
              "      <td>-0.014484</td>\n",
              "      <td>-0.031113</td>\n",
              "      <td>-0.062844</td>\n",
              "      <td>-0.045536</td>\n",
              "      <td>0.012570</td>\n",
              "      <td>0.004928</td>\n",
              "      <td>-0.021738</td>\n",
              "      <td>-0.033730</td>\n",
              "      <td>-0.011367</td>\n",
              "      <td>-0.009055</td>\n",
              "      <td>-0.006117</td>\n",
              "      <td>-0.009460</td>\n",
              "      <td>-0.014713</td>\n",
              "      <td>0.006533</td>\n",
              "      <td>-0.012725</td>\n",
              "      <td>-0.009930</td>\n",
              "      <td>-0.005330</td>\n",
              "      <td>0.006565</td>\n",
              "      <td>-0.014448</td>\n",
              "      <td>-0.012789</td>\n",
              "      <td>-0.008875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.291422</td>\n",
              "      <td>0.777222</td>\n",
              "      <td>-0.408286</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-0.080828</td>\n",
              "      <td>-0.049801</td>\n",
              "      <td>-0.056895</td>\n",
              "      <td>0.019128</td>\n",
              "      <td>-0.070776</td>\n",
              "      <td>-0.027971</td>\n",
              "      <td>0.078002</td>\n",
              "      <td>-0.143408</td>\n",
              "      <td>-0.122654</td>\n",
              "      <td>-0.013020</td>\n",
              "      <td>-0.109531</td>\n",
              "      <td>-0.389553</td>\n",
              "      <td>-0.602799</td>\n",
              "      <td>-0.563673</td>\n",
              "      <td>-0.290784</td>\n",
              "      <td>-0.050571</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.099431</td>\n",
              "      <td>-0.061850</td>\n",
              "      <td>0.024372</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>-0.027266</td>\n",
              "      <td>-0.035613</td>\n",
              "      <td>-0.010752</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.007647</td>\n",
              "      <td>-0.012239</td>\n",
              "      <td>-0.015412</td>\n",
              "      <td>0.007587</td>\n",
              "      <td>-0.011299</td>\n",
              "      <td>-0.009393</td>\n",
              "      <td>-0.007804</td>\n",
              "      <td>0.004307</td>\n",
              "      <td>-0.014111</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>-0.010724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.731807</td>\n",
              "      <td>-0.153182</td>\n",
              "      <td>-0.102195</td>\n",
              "      <td>-0.458238</td>\n",
              "      <td>0.816873</td>\n",
              "      <td>0.046188</td>\n",
              "      <td>0.015792</td>\n",
              "      <td>-0.064744</td>\n",
              "      <td>0.014353</td>\n",
              "      <td>-0.005002</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.017529</td>\n",
              "      <td>-0.007235</td>\n",
              "      <td>0.008140</td>\n",
              "      <td>0.015745</td>\n",
              "      <td>-0.003273</td>\n",
              "      <td>-0.012159</td>\n",
              "      <td>-0.006524</td>\n",
              "      <td>-0.014003</td>\n",
              "      <td>0.005176</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>-0.018706</td>\n",
              "      <td>-0.009773</td>\n",
              "      <td>-0.001108</td>\n",
              "      <td>0.018647</td>\n",
              "      <td>0.005479</td>\n",
              "      <td>-0.018145</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>0.021768</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>-0.013188</td>\n",
              "      <td>0.019870</td>\n",
              "      <td>-0.014564</td>\n",
              "      <td>-0.007068</td>\n",
              "      <td>0.027246</td>\n",
              "      <td>0.025586</td>\n",
              "      <td>-0.017256</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.043840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        37        38        39\n",
              "0 -0.311391  0.786561 -0.420790  ... -0.012853 -0.006433 -0.008562\n",
              "1 -0.284662  0.764035 -0.411982  ... -0.012475 -0.008168 -0.009003\n",
              "2 -0.284659  0.767955 -0.395541  ... -0.014448 -0.012789 -0.008875\n",
              "3 -0.291422  0.777222 -0.408286  ... -0.014111 -0.011770 -0.010724\n",
              "4  0.731807 -0.153182 -0.102195  ... -0.017256  0.000591  0.043840\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T50hJ8mq2AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "85f3d810-10c5-4d9e-c03d-cbac1724d9f2"
      },
      "source": [
        "#%% md\n",
        "####----------------------------------------------------------------\n",
        "#### Split  PCA_Data\n",
        "####----------------------------------------------------------------\n",
        "#%%\n",
        "PCA_X_train, PCA_X_test, PCA_y_train, PCA_y_test  = mySplitData(PCA_Data,Y_Data,0.33,42)\n",
        "\n",
        "PCA_X_train.head()\n",
        "#%%\n",
        "PCA_X_test.head()\n",
        "#%%\n",
        "PCA_y_train.head()\n",
        "#%%\n",
        "PCA_y_test.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22373    0\n",
              "10508    1\n",
              "11570    1\n",
              "22262    0\n",
              "734      1\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaIjFEzq9_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uesuJDbrHRU",
        "colab_type": "text"
      },
      "source": [
        "## **Train and Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nn4NvY4rM6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "7c6d7a04-f864-42dd-c0fd-8ee86ae0b1cd"
      },
      "source": [
        "# configure auto-sklearn\n",
        "anmie_automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "          time_left_for_this_task=120, # run auto-sklearn for at most 2min\n",
        "          per_run_time_limit=30, # spend at most 30 sec for each model training\n",
        "          include_preprocessors=[\"no_preprocessing\"]\n",
        "          )\n",
        "\n",
        "# train model(s)\n",
        "anmie_automl.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "PCA_y_predicted = anmie_automl.predict(PCA_X_test)\n",
        "test_acc = sklearn.metrics.accuracy_score(PCA_y_test, PCA_y_predicted)\n",
        "print(\"Test Accuracy score {0}\".format(test_acc))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 20:08:53,690:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:53,704:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:55,709:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:57,716:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:59,722:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:09:01,737:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:09:03,752:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 20:10:38,380:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
            "[WARNING] [2019-07-29 20:10:38,380:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
            "1\n",
            "['/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000002.ensemble']\n",
            "Test Accuracy score 0.7654299733149694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEjZqYPpswxw",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47ATUCSls-cx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac9221e7-d68b-4dd3-ea09-2074b7421c48"
      },
      "source": [
        "automl.sprint_statistics()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'auto-sklearn results:\\n  Dataset name: d74860caaa557f473ce23908ff7ba369\\n  Metric: accuracy\\n  Best validation score: 0.991011\\n  Number of target algorithm runs: 26\\n  Number of successful target algorithm runs: 22\\n  Number of crashed target algorithm runs: 2\\n  Number of target algorithms that exceeded the time limit: 2\\n  Number of target algorithms that exceeded the memory limit: 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2h1ROVtCWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06dd8475-00f4-4024-9c17-b09cbbbe955f"
      },
      "source": [
        "anmie_automl.show_models()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[(0.640000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'classifier:random_forest:bootstrap': 'True', 'classifier:random_forest:criterion': 'gini', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.5, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 1, 'classifier:random_forest:min_samples_split': 2, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01},\\ndataset_properties={\\n  'task': 1,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': False,\\n  'target_type': 'classification',\\n  'signed': False})),\\n(0.360000, SimpleClassificationPipeline({'balancing:strategy': 'weighting', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'gaussian_nb', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'robust_scaler', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'rescaling:robust_scaler:q_max': 0.8245132980938538, 'rescaling:robust_scaler:q_min': 0.08947420373097192, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00034835629696198427},\\ndataset_properties={\\n  'task': 1,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': False,\\n  'target_type': 'classification',\\n  'signed': False})),\\n]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVYIHHlotGEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bc2bb5e-e277-4d11-afb0-acb6f2af621c"
      },
      "source": [
        "anmie_automl.cv_results_"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([10.36883998, 30.03817606, 30.03083205, 30.03898716,  0.47739506,\n",
              "         3.01575017]),\n",
              " 'mean_test_score': array([0.74174483, 0.        , 0.        , 0.        , 0.65758705,\n",
              "        0.        ]),\n",
              " 'param_balancing:strategy': masked_array(data=['none', 'weighting', 'none', 'none', 'weighting',\n",
              "                    'none'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U9'),\n",
              " 'param_categorical_encoding:__choice__': masked_array(data=['one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U16'),\n",
              " 'param_categorical_encoding:one_hot_encoding:minimum_fraction': masked_array(data=[0.01, 0.010000000000000004, --, --,\n",
              "                    0.00034835629696198427, 0.00012586572428922356],\n",
              "              mask=[False, False,  True,  True, False, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_categorical_encoding:one_hot_encoding:use_minimum_fraction': masked_array(data=['True', 'True', 'False', 'False', 'True', 'True'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U5'),\n",
              " 'param_classifier:__choice__': masked_array(data=['random_forest', 'gradient_boosting', 'libsvm_svc',\n",
              "                    'random_forest', 'gaussian_nb', 'random_forest'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U17'),\n",
              " 'param_classifier:adaboost:algorithm': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:bernoulli_nb:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:bernoulli_nb:fit_prior': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:criterion': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_depth_factor': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_features': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_split': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:bootstrap': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:criterion': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_features': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_samples_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_samples_split': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:gradient_boosting:criterion': masked_array(data=[--, 'mse', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:learning_rate': masked_array(data=[--, 0.051832615669195795, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:loss': masked_array(data=[--, 'deviance', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:max_depth': masked_array(data=[--, 6.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_features': masked_array(data=[--, 0.8807456180216267, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_leaf_nodes': masked_array(data=[--, 'None', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:min_impurity_decrease': masked_array(data=[--, 0.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_leaf': masked_array(data=[--, 7.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_split': masked_array(data=[--, 19.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_weight_fraction_leaf': masked_array(data=[--, 0.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:n_estimators': masked_array(data=[--, 366.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:subsample': masked_array(data=[--, 0.7314831276137047, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:k_nearest_neighbors:n_neighbors': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:k_nearest_neighbors:p': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:k_nearest_neighbors:weights': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:n_components': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:shrinkage': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:shrinkage_factor': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:C': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:dual': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:intercept_scaling': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:multi_class': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:penalty': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:C': masked_array(data=[--, --, 6.342897164595882, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:coef0': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:degree': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:gamma': masked_array(data=[--, --, 0.2229870623330047, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:kernel': masked_array(data=[--, --, 'rbf', --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:max_iter': masked_array(data=[--, --, -1.0, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:shrinking': masked_array(data=[--, --, 'False', --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:tol': masked_array(data=[--, --, 2.006345264381097e-05, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:multinomial_nb:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:multinomial_nb:fit_prior': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:C': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:average': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:qda:reg_param': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:random_forest:bootstrap': masked_array(data=['True', --, --, 'True', --, 'True'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:criterion': masked_array(data=['gini', --, --, 'gini', --, 'gini'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_depth': masked_array(data=['None', --, --, 'None', --, 'None'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_features': masked_array(data=[0.5, --, --, 0.9260795160807372, --,\n",
              "                    0.5240592829918601],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:max_leaf_nodes': masked_array(data=['None', --, --, 'None', --, 'None'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:min_impurity_decrease': masked_array(data=[0.0, --, --, 0.0, --, 0.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_leaf': masked_array(data=[1.0, --, --, 17.0, --, 10.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_split': masked_array(data=[2.0, --, --, 7.0, --, 16.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_weight_fraction_leaf': masked_array(data=[0.0, --, --, 0.0, --, 0.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:n_estimators': masked_array(data=[100.0, --, --, 100.0, --, 100.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:sgd:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:average': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:epsilon': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:eta0': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:l1_ratio': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:penalty': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:power_t': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:base_score': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:booster': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bylevel': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bytree': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:gamma': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_delta_step': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:min_child_weight': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:normalize_type': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:rate_drop': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_lambda': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:sample_type': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:scale_pos_weight': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:subsample': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_imputation:strategy': masked_array(data=['mean', 'mean', 'most_frequent', 'mean', 'mean',\n",
              "                    'mean'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U13'),\n",
              " 'param_preprocessor:__choice__': masked_array(data=['no_preprocessing', 'no_preprocessing',\n",
              "                    'no_preprocessing', 'no_preprocessing',\n",
              "                    'no_preprocessing', 'no_preprocessing'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U16'),\n",
              " 'param_rescaling:__choice__': masked_array(data=['standardize', 'standardize', 'standardize', 'minmax',\n",
              "                    'robust_scaler', 'normalize'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U13'),\n",
              " 'param_rescaling:quantile_transformer:n_quantiles': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_rescaling:quantile_transformer:output_distribution': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_rescaling:robust_scaler:q_max': masked_array(data=[--, --, --, --, 0.8245132980938538, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_rescaling:robust_scaler:q_min': masked_array(data=[--, --, --, --, 0.08947420373097192, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 1,\n",
              "   'classifier:random_forest:min_samples_split': 2,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.010000000000000004,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.051832615669195795,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 6,\n",
              "   'classifier:gradient_boosting:max_features': 0.8807456180216267,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 7,\n",
              "   'classifier:gradient_boosting:min_samples_split': 19,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 366,\n",
              "   'classifier:gradient_boosting:subsample': 0.7314831276137047,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'libsvm_svc',\n",
              "   'classifier:libsvm_svc:C': 6.342897164595882,\n",
              "   'classifier:libsvm_svc:gamma': 0.2229870623330047,\n",
              "   'classifier:libsvm_svc:kernel': 'rbf',\n",
              "   'classifier:libsvm_svc:max_iter': -1,\n",
              "   'classifier:libsvm_svc:shrinking': 'False',\n",
              "   'classifier:libsvm_svc:tol': 2.006345264381097e-05,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.9260795160807372,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 17,\n",
              "   'classifier:random_forest:min_samples_split': 7,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00034835629696198427,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gaussian_nb',\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8245132980938538,\n",
              "   'rescaling:robust_scaler:q_min': 0.08947420373097192},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00012586572428922356,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5240592829918601,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 10,\n",
              "   'classifier:random_forest:min_samples_split': 16,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'normalize'}],\n",
              " 'rank_test_scores': array([1, 3, 3, 3, 2, 3]),\n",
              " 'status': ['Success', 'Timeout', 'Timeout', 'Timeout', 'Success', 'Timeout']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhpbbHnV3b3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0nlmBEQR6Nd",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuHZbULXSTfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c072386e-d77c-4c66-bb01-2b0259e091be"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "\n",
        "# train model(s)\n",
        "knn_m = knn.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "knn_test_acc = knn_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(knn_test_acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7884996126366531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcFpZiIR87D",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJiVRVCXvKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca79ce50-77cd-49c8-ca03-73e35e7bc148"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "# Create a svm Classifier with PCA data\n",
        "svc = svm.SVC(C=1.0, gamma=0.1, kernel='rbf') # Linear Kernel\n",
        "\n",
        "# train model(s)\n",
        "svm_m = svc.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "svm_test_acc = svm_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(svm_test_acc))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.6978565894809331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WefgYsW4R_td",
        "colab_type": "text"
      },
      "source": [
        "# DT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmpigibbYm2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25e78fa6-3d6b-4402-c45f-148cf277710f"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "\n",
        "\n",
        "# train model(s)\n",
        "dt_m = dt.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "dt_test_acc = dt_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(dt_test_acc))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.794525264698287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zntQsupZSB_O",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N_gyiFQaL-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6629eab6-fdb5-4442-e188-ea54ab4f3f8f"
      },
      "source": [
        "#Import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier with original data\n",
        "rf = RandomForestClassifier(criterion ='gini', max_depth= 15, max_features= 'sqrt', min_samples_leaf=1, min_samples_split= 5, n_estimators= 300)\n",
        "\n",
        "\n",
        "# train model(s)\n",
        "rf_m = rf.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "rf_test_acc = rf_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(rf_test_acc))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7976241714728415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eToqzWdTSFFB",
        "colab_type": "text"
      },
      "source": [
        "# Neuron Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkdWo0y0h5PZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1302795-8541-4a89-825a-63670f98bb97"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create a NN Classifier with PCA data\n",
        "nn = MLPClassifier(max_iter=500)\n",
        "\n",
        "# train model(s)\n",
        "nn_m = nn.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "nn_test_acc = nn_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(nn_test_acc))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7011276577429629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z8eT3iobBY8",
        "colab_type": "text"
      },
      "source": [
        "# Comparision "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wXnM5YYecEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "c1120d58-ae80-4a95-cc36-b071a728f981"
      },
      "source": [
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#import autosklearn.classification\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "\n",
        "def roc_auc_curve(model,X_train,Y_train,X_test,Y_test):\n",
        "  model.fit(X_train, Y_train)\n",
        "\n",
        "  dt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "  dt_lm.fit(X_test, Y_test)\n",
        "\n",
        "  y_pred_dt = model.predict_proba(X_test)[:, 1]\n",
        "  fpr_dt, tpr_dt, _ = roc_curve(Y_test, y_pred_dt)\n",
        "  roc_auc = auc(fpr_dt, tpr_dt)\n",
        "  return fpr_dt, tpr_dt,roc_auc\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "\n",
        "#clf.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "#dt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "#dt_lm.fit(PCA_X_test, PCA_y_test)\n",
        "\n",
        "#y_pred_dt = clf.predict_proba(PCA_X_test)[:, 1]\n",
        "#fpr_dt, tpr_dt, _ = roc_curve(PCA_y_test, y_pred_dt)\n",
        "#roc_auc = auc(fpr_dt, tpr_dt)\n",
        "\n",
        "fpr_dt, tpr_dt,roc_auc = roc_auc_curve(clf,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "\n",
        "#KNN \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "knn_m = knn.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "knn_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "knn_lm.fit(PCA_X_test, PCA_y_test)\n",
        "\n",
        "y_pred_knn = knn.predict_proba(PCA_X_test)[:, 1]\n",
        "knn_fpr, knn_tpr, _ = roc_curve(PCA_y_test, y_pred_knn)\n",
        "knn_roc_auc = auc(knn_fpr, knn_tpr)\n",
        "\n",
        "#Random Forest \n",
        "rf = RandomForestClassifier(criterion ='gini', max_depth= 15, max_features= 'sqrt', min_samples_leaf=1, min_samples_split= 5, n_estimators= 300)\n",
        "\n",
        "rf_m = rf.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "rf_lm.fit(PCA_X_test, PCA_y_test)\n",
        "\n",
        "rf_y_pred = rf.predict_proba(PCA_X_test)[:, 1]\n",
        "rf_fpr, rf_tpr, _ = roc_curve(PCA_y_test, rf_y_pred)\n",
        "rf_roc_auc = auc(rf_fpr, rf_tpr)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "\n",
        "plt.plot(fpr_dt, tpr_dt, label='ROC of DT (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot(knn_fpr_dt, knn_tpr_dt, label='ROC of KNN (AUC = %0.2f)' % knn_roc_auc)\n",
        "plt.plot(rf_fpr, rf_tpr, label='ROC of RF (AUC = %0.2f)' % rf_roc_auc)\n",
        "\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQpQykryFEEKIUub/AREXwudIoLQoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyllWO0npzLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "9f7a5833-75dd-4414-823f-b875288eadd0"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import autosklearn.classification\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "\n",
        "def roc_auc_curve(model,X_train,Y_train,X_test,Y_test):\n",
        "  model.fit(X_train, Y_train)\n",
        "\n",
        "  dt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "  dt_lm.fit(X_test, Y_test)\n",
        "\n",
        "  y_pred_dt = model.predict_proba(X_test)[:, 1]\n",
        "  fpr_dt, tpr_dt, _ = roc_curve(Y_test, y_pred_dt)\n",
        "  roc_auc = auc(fpr_dt, tpr_dt)\n",
        "  return fpr_dt, tpr_dt,roc_auc\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "fpr_dt, tpr_dt,roc_auc = roc_auc_curve(clf,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "\n",
        "#KNN \n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "knn_fpr, knn_tpr,knn_roc_auc = roc_auc_curve(knn,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "#Random Forest \n",
        "rf = RandomForestClassifier(criterion ='gini', max_depth= 15, max_features= 'sqrt', min_samples_leaf=1, min_samples_split= 5, n_estimators= 300)\n",
        "rf_fpr, rf_tpr,rf_roc_auc = roc_auc_curve(rf,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "#svm \n",
        "svc = svm.SVC(C=1.0, gamma=0.1, kernel='rbf') # Linear Kernel\n",
        "svm_fpr, svm_tpr,svm_roc_auc = roc_auc_curve(svm,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "#NN \n",
        "nn = MLPClassifier(max_iter=500)\n",
        "nn_fpr, nn_tpr,nn_roc_auc = roc_auc_curve(nn,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "#AutoML \n",
        "AutoML = autosklearn.classification.AutoSklearnClassifier(\n",
        "          time_left_for_this_task=120, # run auto-sklearn for at most 2min\n",
        "          per_run_time_limit=30, # spend at most 30 sec for each model training\n",
        "          include_preprocessors=[\"no_preprocessing\"]\n",
        "          )\n",
        "AutoML_fpr, AutoML_tpr,AutoML_roc_auc = roc_auc_curve(AutoML,PCA_X_train,PCA_y_train,PCA_X_test,PCA_y_test)\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "\n",
        "plt.plot(fpr_dt, tpr_dt, label='ROC of DT (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot(knn_fpr_dt, knn_tpr_dt, label='ROC of KNN (AUC = %0.2f)' % knn_roc_auc)\n",
        "plt.plot(rf_fpr, rf_tpr, label='ROC of RF (AUC = %0.2f)' % rf_roc_auc)\n",
        "plt.plot(svm_fpr, svm_rf_tpr, label='ROC of SVM (AUC = %0.2f)' % svm_roc_auc)\n",
        "plt.plot(nn_fpr, nn_rf_tpr, label='ROC of NN (AUC = %0.2f)' % nn_roc_auc)\n",
        "plt.plot(AutoML_fpr, AutoML_rf_tpr, label='ROC of NN (AUC = %0.2f)' % AutoML_roc_auc)\n",
        "\n",
        "\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-0a309719c30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#svm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Linear Kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0msvm_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_tpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_roc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPCA_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPCA_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPCA_X_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPCA_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-0a309719c30b>\u001b[0m in \u001b[0;36mroc_auc_curve\u001b[0;34m(model, X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mroc_auc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mdt_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.svm' has no attribute 'fit'"
          ]
        }
      ]
    }
  ]
}