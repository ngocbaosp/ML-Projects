{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoMLProject01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "StxbZGOO2y3I"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngocbaosp/ML-Projects/blob/master/AutoMLProject01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StxbZGOO2y3I",
        "colab_type": "text"
      },
      "source": [
        "# Install AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BisgUVyh-3x",
        "colab_type": "code",
        "outputId": "327fcbc9-40fc-4f14-d1e8-4bf7a162040c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,288 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 131331 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Collecting auto-sklearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/99/27caac4f6804be48722158e31c630e0737110581774df0615a36b21239aa/auto-sklearn-0.5.2.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.12)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Collecting scikit-learn<0.20,>=0.19 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Collecting lockfile (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Collecting liac-arff (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Collecting ConfigSpace<0.5,>=0.4.0 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/de/4e8e4f26332fc65404f52baa112defbf822b6738b60bfa6b2993f5c60933/ConfigSpace-0.4.10.tar.gz (882kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.0MB/s \n",
            "\u001b[?25hCollecting pynisher>=0.4.2 (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/21/c86c64c305da6d43fb89780d33cbc839c07736b71955a8bdb642a02b7538/pynisher-0.5.0.tar.gz\n",
            "Collecting pyrfr<0.8,>=0.7 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/c6/c555cfa3c7d0078dded091d4901ed52344bbb925077aa70b871faf35fd58/pyrfr-0.7.4.tar.gz (291kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 42.5MB/s \n",
            "\u001b[?25hCollecting smac==0.8 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ab/2b0a6fb00bd76e2415a04dcca453ad0b0db9b4218b02401306ff2bc6135d/smac-0.8.0.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.7.4)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Collecting sphinx_rtd_theme (from smac==0.8->auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/4df37087a1d36755e3a3bfd2a30263f358d2dea21938240fa02313d45f51/sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4MB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.7.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Building wheels for collected packages: auto-sklearn, liac-arff, ConfigSpace, pynisher, pyrfr, smac\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/21/43/182fed664b6474f88600c110c4ebd254d6256ba59175cef3fd\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
            "  Building wheel for ConfigSpace (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/83/cb/28dd42bac69c8867d485138030daa83841c7f84afe68b2fdf7\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2a/c4/ec3abc8a2f786ef9786ea8fe6ff629a4e54812a3f98cc41b47\n",
            "  Building wheel for pyrfr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/98/fd/b1d53cab6d5ed836980777d9733d7e549d82a727650eed6f6d\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/52/83/d2d66a840968025d072ddb1cd776fdc5eb5e337e1cc887bc3f\n",
            "Successfully built auto-sklearn liac-arff ConfigSpace pynisher pyrfr smac\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, lockfile, liac-arff, ConfigSpace, pynisher, pyrfr, sphinx-rtd-theme, smac, auto-sklearn\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "Successfully installed ConfigSpace-0.4.10 auto-sklearn-0.5.2 liac-arff-2.4.0 lockfile-0.12.2 pynisher-0.5.0 pyrfr-0.7.4 scikit-learn-0.19.2 smac-0.8.0 sphinx-rtd-theme-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Z7JRDni3Ms",
        "colab_type": "code",
        "outputId": "d7a609d3-4e7f-4e6a-d8ac-8e98fead1b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install auto-sklearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.2)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.19.2)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.4.10)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: pyrfr<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.7.4)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.12)\n",
            "Requirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Requirement already satisfied: smac==0.8 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.8.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.7.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (0.4.3)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.7.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9o5GpGi7e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "\n",
        "# Load data\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3zsYPyooEVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE4j6toFjO",
        "colab_type": "code",
        "outputId": "4d7123e2-39fb-486b-cca2-1e91328e031f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import autosklearn.classification\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVxpdnVXjA-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d73769c2-bdd1-4761-de18-56be93875412"
      },
      "source": [
        "\n",
        "# configure auto-sklearn\n",
        "automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "          time_left_for_this_task=120, # run auto-sklearn for at most 2min\n",
        "          per_run_time_limit=30, # spend at most 30 sec for each model training\n",
        "          )\n",
        "\n",
        "# train model(s)\n",
        "automl.fit(X_train, y_train)\n",
        "\n",
        "# evaluate\n",
        "y_hat = automl.predict(X_test)\n",
        "test_acc = sklearn.metrics.accuracy_score(y_test, y_hat)\n",
        "print(\"Test Accuracy score {0}\".format(test_acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 19:46:10,678:EnsembleBuilder(1):d74860caaa557f473ce23908ff7ba369] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 19:46:10,696:EnsembleBuilder(1):d74860caaa557f473ce23908ff7ba369] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "Process pynisher function call:\n",
            "Process pynisher function call:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/__init__.py\", line 30, in fit_predict_try_except_decorator\n",
            "    return ta(queue=queue, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/ensemble_builder.py\", line 234, in main\n",
            "    time.sleep(self.sleep_duration)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 648, in eval_holdout\n",
            "    evaluator.fit_predict_and_loss(iterative=iterative)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 160, in fit_predict_and_loss\n",
            "    i, train_indices=train_split, test_indices=test_split\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 406, in _partial_fit_and_predict\n",
            "    self.Y_train[train_indices])\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/abstract_evaluator.py\", line 481, in _fit_and_suppress_warnings\n",
            "    model.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 93, in fit\n",
            "    self.fit_estimator(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 110, in fit_estimator\n",
            "    self._final_estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 402, in fit\n",
            "    return self.choice.fit(X, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 163, in fit\n",
            "    self.iterative_fit(X, y, n_iter=n_iter, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/classification/gradient_boosting.py\", line 90, in iterative_fit\n",
            "    self.estimator.fit(X, y, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1034, in fit\n",
            "    begin_at_stage, monitor, X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1089, in _fit_stages\n",
            "    X_csc, X_csr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 798, in _fit_stage\n",
            "    self.learning_rate, k=k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 249, in update_terminal_regions\n",
            "    y_pred[:, k], sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 572, in _update_terminal_region\n",
            "    numerator = np.sum(sample_weight * residual)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 2076, in sum\n",
            "    initial=initial)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\n",
            "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "KeyboardInterrupt\n",
            "Process pynisher function call:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
            "    return_value = ((func(*args, **kwargs), 0))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/__init__.py\", line 30, in fit_predict_try_except_decorator\n",
            "    return ta(queue=queue, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 648, in eval_holdout\n",
            "    evaluator.fit_predict_and_loss(iterative=iterative)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 160, in fit_predict_and_loss\n",
            "    i, train_indices=train_split, test_indices=test_split\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py\", line 406, in _partial_fit_and_predict\n",
            "    self.Y_train[train_indices])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/abstract_evaluator.py\", line 481, in _fit_and_suppress_warnings\n",
            "    model.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 93, in fit\n",
            "    self.fit_estimator(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/base.py\", line 110, in fit_estimator\n",
            "    self._final_estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 402, in fit\n",
            "    return self.choice.fit(X, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/base.py\", line 163, in fit\n",
            "    self.iterative_fit(X, y, n_iter=n_iter, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/autosklearn/pipeline/components/classification/gradient_boosting.py\", line 90, in iterative_fit\n",
            "    self.estimator.fit(X, y, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1034, in fit\n",
            "    begin_at_stage, monitor, X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 1089, in _fit_stages\n",
            "    X_csc, X_csr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\", line 788, in _fit_stage\n",
            "    check_input=False, X_idx_sorted=X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\", line 1124, in fit\n",
            "    X_idx_sorted=X_idx_sorted)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\", line 362, in fit\n",
            "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
            "KeyboardInterrupt\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_132_977/.auto-sklearn/ensembles/1.0000000003.ensemble']\n",
            "Test Accuracy score 0.9933333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X89LTFhoI2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.sprint_statistics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukJU3_roQjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.show_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmixqkcMoav8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "automl.cv_results_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GjDTVskomp1",
        "colab_type": "text"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eWSIgFWo8O2",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a59x2kHcpQv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXK2-zGSpCxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% md\n",
        "#### MyPCA\n",
        "#%%\n",
        "def myPCA(data,n):\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(data)\n",
        "    df = pca.transform(data)\n",
        "    PCA_Data = pd.DataFrame(df)\n",
        "    return PCA_Data\n",
        "\n",
        "#%% md\n",
        "#### myNormalize\n",
        "#%%\n",
        "def myNormalize(data):\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    Normalized_Data = min_max_scaler.fit_transform(data)\n",
        "    Normalized_Data = pd.DataFrame(Normalized_Data)\n",
        "    return Normalized_Data\n",
        "\n",
        "#%% md\n",
        "#### myEncode\n",
        "#%%\n",
        "def myEncode(data,col): \n",
        "    NewData_Encode = data.copy()\n",
        "    NewData_Encode = pd.get_dummies(NewData_Encode, columns=col, prefix = col)\n",
        "    return NewData_Encode\n",
        "\n",
        "\n",
        "#%% md\n",
        "#### myCleanAndTransformData\n",
        "#%%\n",
        "def myCleanAndTransformData(data):\n",
        "    \n",
        "    #Drop null rows\n",
        "    NewData = data.dropna()\n",
        "    #Remove unknown ata\n",
        "    NewData = NewData[NewData['episodes']!='Unknown']\n",
        "    #Add a new column rating class \n",
        "    NewData['Class']=1\n",
        "    # 1: High\n",
        "    # or 0: Low based on rating\n",
        "    NewData.loc[NewData['rating'] >= NewData['rating'].mean(), 'Class'] = 1\n",
        "    NewData.loc[NewData['rating'] < NewData['rating'].mean(), 'Class'] = 0\n",
        "    \n",
        "    #Split genre values into rows\n",
        "    NewData = pd.DataFrame(NewData.genre.str.split(',').tolist(), index=[NewData.anime_id,NewData.type,NewData.episodes,NewData.rating,NewData.members,NewData.Class]).stack()\n",
        "    NewData = NewData.reset_index([0,'anime_id','type','episodes','rating','members','Class'])\n",
        "    NewData.columns=['anime_id','type','episodes','rating','members','Class','genre']\n",
        "    \n",
        "    #Encode type feature: 6 unique values\n",
        "    NewData = myEncode(NewData,['type'])\n",
        " \n",
        "    #Encode genre feature: 82 unique values\n",
        "    NewData = myEncode(NewData,['genre'])\n",
        " \n",
        "     #Drop anmie_id,rating,Class\n",
        "    NewData = NewData.drop(['rating'],axis=1)\n",
        "    NewData = NewData.drop(columns=['anime_id'])\n",
        "    NewData = NewData.drop(columns=['episodes'])  \n",
        "    \n",
        "    return NewData\n",
        "\n",
        "\n",
        "#%% md\n",
        "#### mySplitData\n",
        "#%%\n",
        "def mySplitData(X_Data,Y_Data,test_size,random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def mySplitDataByTrainSize(X_Data,Y_Data,train_size,random_state):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, train_size=train_size, random_state=random_state)\n",
        "    X_train, X_test, y_train, y_test = mySplitData(X_train,y_train,0.33,random_state)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWwslp1EpaUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bbc6631e-d9e7-4cc5-b5ea-996955e41ad2"
      },
      "source": [
        "#%% md\n",
        "# Load data from files\n",
        "#%%\n",
        "RawData = pd.read_csv('anime.csv')\n",
        "RawData.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anime_id</th>\n",
              "      <th>name</th>\n",
              "      <th>genre</th>\n",
              "      <th>type</th>\n",
              "      <th>episodes</th>\n",
              "      <th>rating</th>\n",
              "      <th>members</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32281</td>\n",
              "      <td>Kimi no Na wa.</td>\n",
              "      <td>Drama, Romance, School, Supernatural</td>\n",
              "      <td>Movie</td>\n",
              "      <td>1</td>\n",
              "      <td>9.37</td>\n",
              "      <td>200630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5114</td>\n",
              "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
              "      <td>Action, Adventure, Drama, Fantasy, Magic, Mili...</td>\n",
              "      <td>TV</td>\n",
              "      <td>64</td>\n",
              "      <td>9.26</td>\n",
              "      <td>793665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28977</td>\n",
              "      <td>Gintama°</td>\n",
              "      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n",
              "      <td>TV</td>\n",
              "      <td>51</td>\n",
              "      <td>9.25</td>\n",
              "      <td>114262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9253</td>\n",
              "      <td>Steins;Gate</td>\n",
              "      <td>Sci-Fi, Thriller</td>\n",
              "      <td>TV</td>\n",
              "      <td>24</td>\n",
              "      <td>9.17</td>\n",
              "      <td>673572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9969</td>\n",
              "      <td>Gintama&amp;#039;</td>\n",
              "      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n",
              "      <td>TV</td>\n",
              "      <td>51</td>\n",
              "      <td>9.16</td>\n",
              "      <td>151266</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   anime_id                              name  ... rating members\n",
              "0     32281                    Kimi no Na wa.  ...   9.37  200630\n",
              "1      5114  Fullmetal Alchemist: Brotherhood  ...   9.26  793665\n",
              "2     28977                          Gintama°  ...   9.25  114262\n",
              "3      9253                       Steins;Gate  ...   9.17  673572\n",
              "4      9969                     Gintama&#039;  ...   9.16  151266\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwC0YFqhptN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b28792a5-aa2c-47ec-ea4a-9a0b24073f87"
      },
      "source": [
        "#%% md\n",
        "#### Clean and Transform Data\n",
        "#%%\n",
        "Cleaned_Data = myCleanAndTransformData(RawData)\n",
        "Y_Data = Cleaned_Data['Class']\n",
        "X_Data = Cleaned_Data.drop(columns=['Class'])\n",
        "\n",
        "#%% md\n",
        "#### Normalize  Data\n",
        "#%%\n",
        "Normalized_Data = myNormalize(X_Data)\n",
        "#%% md\n",
        "#### PCA\n",
        "#%%\n",
        "n_components=40\n",
        "PCA_Data = myPCA(Normalized_Data,n_components)\n",
        "PCA_Data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.311391</td>\n",
              "      <td>0.786561</td>\n",
              "      <td>-0.420790</td>\n",
              "      <td>0.005234</td>\n",
              "      <td>-0.078663</td>\n",
              "      <td>-0.049646</td>\n",
              "      <td>-0.062640</td>\n",
              "      <td>0.007159</td>\n",
              "      <td>-0.075350</td>\n",
              "      <td>-0.030938</td>\n",
              "      <td>0.086247</td>\n",
              "      <td>-0.139425</td>\n",
              "      <td>-0.157022</td>\n",
              "      <td>0.028365</td>\n",
              "      <td>-0.081108</td>\n",
              "      <td>-0.232703</td>\n",
              "      <td>-0.299120</td>\n",
              "      <td>0.804699</td>\n",
              "      <td>-0.258797</td>\n",
              "      <td>-0.007687</td>\n",
              "      <td>-0.094831</td>\n",
              "      <td>-0.108054</td>\n",
              "      <td>-0.062475</td>\n",
              "      <td>0.025711</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>-0.024983</td>\n",
              "      <td>-0.033523</td>\n",
              "      <td>-0.004934</td>\n",
              "      <td>-0.011702</td>\n",
              "      <td>-0.006729</td>\n",
              "      <td>-0.011742</td>\n",
              "      <td>-0.014507</td>\n",
              "      <td>0.009433</td>\n",
              "      <td>-0.010485</td>\n",
              "      <td>-0.008221</td>\n",
              "      <td>-0.004127</td>\n",
              "      <td>0.005306</td>\n",
              "      <td>-0.012853</td>\n",
              "      <td>-0.006433</td>\n",
              "      <td>-0.008562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.284662</td>\n",
              "      <td>0.764035</td>\n",
              "      <td>-0.411982</td>\n",
              "      <td>-0.010872</td>\n",
              "      <td>-0.110067</td>\n",
              "      <td>-0.087035</td>\n",
              "      <td>-0.096786</td>\n",
              "      <td>0.054585</td>\n",
              "      <td>-0.179466</td>\n",
              "      <td>-0.045549</td>\n",
              "      <td>0.764383</td>\n",
              "      <td>0.581495</td>\n",
              "      <td>0.033808</td>\n",
              "      <td>-0.066725</td>\n",
              "      <td>0.030970</td>\n",
              "      <td>0.068166</td>\n",
              "      <td>0.010372</td>\n",
              "      <td>-0.031955</td>\n",
              "      <td>-0.043632</td>\n",
              "      <td>0.008161</td>\n",
              "      <td>-0.027471</td>\n",
              "      <td>-0.040192</td>\n",
              "      <td>-0.033360</td>\n",
              "      <td>0.004737</td>\n",
              "      <td>0.006610</td>\n",
              "      <td>-0.016134</td>\n",
              "      <td>-0.028708</td>\n",
              "      <td>-0.009434</td>\n",
              "      <td>-0.005291</td>\n",
              "      <td>-0.004034</td>\n",
              "      <td>-0.007216</td>\n",
              "      <td>-0.013007</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>-0.011350</td>\n",
              "      <td>-0.008301</td>\n",
              "      <td>-0.001372</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>-0.012475</td>\n",
              "      <td>-0.008168</td>\n",
              "      <td>-0.009003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.284659</td>\n",
              "      <td>0.767955</td>\n",
              "      <td>-0.395541</td>\n",
              "      <td>-0.007615</td>\n",
              "      <td>-0.091869</td>\n",
              "      <td>-0.059768</td>\n",
              "      <td>-0.062093</td>\n",
              "      <td>0.036484</td>\n",
              "      <td>-0.086830</td>\n",
              "      <td>-0.024722</td>\n",
              "      <td>0.092133</td>\n",
              "      <td>-0.282600</td>\n",
              "      <td>-0.452072</td>\n",
              "      <td>-0.568979</td>\n",
              "      <td>0.527521</td>\n",
              "      <td>0.286982</td>\n",
              "      <td>0.035407</td>\n",
              "      <td>-0.060924</td>\n",
              "      <td>-0.105755</td>\n",
              "      <td>-0.014484</td>\n",
              "      <td>-0.031113</td>\n",
              "      <td>-0.062844</td>\n",
              "      <td>-0.045536</td>\n",
              "      <td>0.012570</td>\n",
              "      <td>0.004928</td>\n",
              "      <td>-0.021738</td>\n",
              "      <td>-0.033730</td>\n",
              "      <td>-0.011367</td>\n",
              "      <td>-0.009055</td>\n",
              "      <td>-0.006117</td>\n",
              "      <td>-0.009460</td>\n",
              "      <td>-0.014713</td>\n",
              "      <td>0.006533</td>\n",
              "      <td>-0.012725</td>\n",
              "      <td>-0.009930</td>\n",
              "      <td>-0.005330</td>\n",
              "      <td>0.006565</td>\n",
              "      <td>-0.014448</td>\n",
              "      <td>-0.012789</td>\n",
              "      <td>-0.008875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.291422</td>\n",
              "      <td>0.777222</td>\n",
              "      <td>-0.408286</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-0.080828</td>\n",
              "      <td>-0.049801</td>\n",
              "      <td>-0.056895</td>\n",
              "      <td>0.019128</td>\n",
              "      <td>-0.070776</td>\n",
              "      <td>-0.027971</td>\n",
              "      <td>0.078002</td>\n",
              "      <td>-0.143408</td>\n",
              "      <td>-0.122654</td>\n",
              "      <td>-0.013020</td>\n",
              "      <td>-0.109531</td>\n",
              "      <td>-0.389553</td>\n",
              "      <td>-0.602799</td>\n",
              "      <td>-0.563673</td>\n",
              "      <td>-0.290784</td>\n",
              "      <td>-0.050571</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.099431</td>\n",
              "      <td>-0.061850</td>\n",
              "      <td>0.024372</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>-0.027266</td>\n",
              "      <td>-0.035613</td>\n",
              "      <td>-0.010752</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.007647</td>\n",
              "      <td>-0.012239</td>\n",
              "      <td>-0.015412</td>\n",
              "      <td>0.007587</td>\n",
              "      <td>-0.011299</td>\n",
              "      <td>-0.009393</td>\n",
              "      <td>-0.007804</td>\n",
              "      <td>0.004307</td>\n",
              "      <td>-0.014111</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>-0.010724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.731807</td>\n",
              "      <td>-0.153182</td>\n",
              "      <td>-0.102195</td>\n",
              "      <td>-0.458238</td>\n",
              "      <td>0.816873</td>\n",
              "      <td>0.046188</td>\n",
              "      <td>0.015792</td>\n",
              "      <td>-0.064744</td>\n",
              "      <td>0.014353</td>\n",
              "      <td>-0.005002</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.017529</td>\n",
              "      <td>-0.007235</td>\n",
              "      <td>0.008140</td>\n",
              "      <td>0.015745</td>\n",
              "      <td>-0.003273</td>\n",
              "      <td>-0.012159</td>\n",
              "      <td>-0.006524</td>\n",
              "      <td>-0.014003</td>\n",
              "      <td>0.005176</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>-0.018706</td>\n",
              "      <td>-0.009773</td>\n",
              "      <td>-0.001108</td>\n",
              "      <td>0.018647</td>\n",
              "      <td>0.005479</td>\n",
              "      <td>-0.018145</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>0.021768</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>-0.013188</td>\n",
              "      <td>0.019870</td>\n",
              "      <td>-0.014564</td>\n",
              "      <td>-0.007068</td>\n",
              "      <td>0.027246</td>\n",
              "      <td>0.025586</td>\n",
              "      <td>-0.017256</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.043840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        37        38        39\n",
              "0 -0.311391  0.786561 -0.420790  ... -0.012853 -0.006433 -0.008562\n",
              "1 -0.284662  0.764035 -0.411982  ... -0.012475 -0.008168 -0.009003\n",
              "2 -0.284659  0.767955 -0.395541  ... -0.014448 -0.012789 -0.008875\n",
              "3 -0.291422  0.777222 -0.408286  ... -0.014111 -0.011770 -0.010724\n",
              "4  0.731807 -0.153182 -0.102195  ... -0.017256  0.000591  0.043840\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T50hJ8mq2AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "85f3d810-10c5-4d9e-c03d-cbac1724d9f2"
      },
      "source": [
        "#%% md\n",
        "####----------------------------------------------------------------\n",
        "#### Split  PCA_Data\n",
        "####----------------------------------------------------------------\n",
        "#%%\n",
        "PCA_X_train, PCA_X_test, PCA_y_train, PCA_y_test  = mySplitData(PCA_Data,Y_Data,0.33,42)\n",
        "\n",
        "PCA_X_train.head()\n",
        "#%%\n",
        "PCA_X_test.head()\n",
        "#%%\n",
        "PCA_y_train.head()\n",
        "#%%\n",
        "PCA_y_test.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22373    0\n",
              "10508    1\n",
              "11570    1\n",
              "22262    0\n",
              "734      1\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaIjFEzq9_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uesuJDbrHRU",
        "colab_type": "text"
      },
      "source": [
        "## **Train and Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nn4NvY4rM6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "7c6d7a04-f864-42dd-c0fd-8ee86ae0b1cd"
      },
      "source": [
        "# configure auto-sklearn\n",
        "anmie_automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "          time_left_for_this_task=120, # run auto-sklearn for at most 2min\n",
        "          per_run_time_limit=30, # spend at most 30 sec for each model training\n",
        "          include_preprocessors=[\"no_preprocessing\"]\n",
        "          )\n",
        "\n",
        "# train model(s)\n",
        "anmie_automl.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "PCA_y_predicted = anmie_automl.predict(PCA_X_test)\n",
        "test_acc = sklearn.metrics.accuracy_score(PCA_y_test, PCA_y_predicted)\n",
        "print(\"Test Accuracy score {0}\".format(test_acc))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 20:08:53,690:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:53,704:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:55,709:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:57,716:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:08:59,722:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:09:01,737:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-07-29 20:09:03,752:EnsembleBuilder(1):71f0d138d36d804a44a29370a5721519] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-07-29 20:10:38,380:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
            "[WARNING] [2019-07-29 20:10:38,380:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
            "1\n",
            "['/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_132_195/.auto-sklearn/ensembles/1.0000000002.ensemble']\n",
            "Test Accuracy score 0.7654299733149694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEjZqYPpswxw",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47ATUCSls-cx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac9221e7-d68b-4dd3-ea09-2074b7421c48"
      },
      "source": [
        "automl.sprint_statistics()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'auto-sklearn results:\\n  Dataset name: d74860caaa557f473ce23908ff7ba369\\n  Metric: accuracy\\n  Best validation score: 0.991011\\n  Number of target algorithm runs: 26\\n  Number of successful target algorithm runs: 22\\n  Number of crashed target algorithm runs: 2\\n  Number of target algorithms that exceeded the time limit: 2\\n  Number of target algorithms that exceeded the memory limit: 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2h1ROVtCWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06dd8475-00f4-4024-9c17-b09cbbbe955f"
      },
      "source": [
        "anmie_automl.show_models()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[(0.640000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'classifier:random_forest:bootstrap': 'True', 'classifier:random_forest:criterion': 'gini', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.5, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 1, 'classifier:random_forest:min_samples_split': 2, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01},\\ndataset_properties={\\n  'task': 1,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': False,\\n  'target_type': 'classification',\\n  'signed': False})),\\n(0.360000, SimpleClassificationPipeline({'balancing:strategy': 'weighting', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'gaussian_nb', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'robust_scaler', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'rescaling:robust_scaler:q_max': 0.8245132980938538, 'rescaling:robust_scaler:q_min': 0.08947420373097192, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00034835629696198427},\\ndataset_properties={\\n  'task': 1,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': False,\\n  'target_type': 'classification',\\n  'signed': False})),\\n]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVYIHHlotGEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bc2bb5e-e277-4d11-afb0-acb6f2af621c"
      },
      "source": [
        "anmie_automl.cv_results_"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([10.36883998, 30.03817606, 30.03083205, 30.03898716,  0.47739506,\n",
              "         3.01575017]),\n",
              " 'mean_test_score': array([0.74174483, 0.        , 0.        , 0.        , 0.65758705,\n",
              "        0.        ]),\n",
              " 'param_balancing:strategy': masked_array(data=['none', 'weighting', 'none', 'none', 'weighting',\n",
              "                    'none'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U9'),\n",
              " 'param_categorical_encoding:__choice__': masked_array(data=['one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U16'),\n",
              " 'param_categorical_encoding:one_hot_encoding:minimum_fraction': masked_array(data=[0.01, 0.010000000000000004, --, --,\n",
              "                    0.00034835629696198427, 0.00012586572428922356],\n",
              "              mask=[False, False,  True,  True, False, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_categorical_encoding:one_hot_encoding:use_minimum_fraction': masked_array(data=['True', 'True', 'False', 'False', 'True', 'True'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U5'),\n",
              " 'param_classifier:__choice__': masked_array(data=['random_forest', 'gradient_boosting', 'libsvm_svc',\n",
              "                    'random_forest', 'gaussian_nb', 'random_forest'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U17'),\n",
              " 'param_classifier:adaboost:algorithm': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:adaboost:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:bernoulli_nb:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:bernoulli_nb:fit_prior': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:criterion': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_depth_factor': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_features': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_split': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:bootstrap': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:criterion': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_features': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_samples_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_samples_split': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:gradient_boosting:criterion': masked_array(data=[--, 'mse', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:learning_rate': masked_array(data=[--, 0.051832615669195795, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:loss': masked_array(data=[--, 'deviance', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:max_depth': masked_array(data=[--, 6.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_features': masked_array(data=[--, 0.8807456180216267, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_leaf_nodes': masked_array(data=[--, 'None', --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:min_impurity_decrease': masked_array(data=[--, 0.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_leaf': masked_array(data=[--, 7.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_split': masked_array(data=[--, 19.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_weight_fraction_leaf': masked_array(data=[--, 0.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:n_estimators': masked_array(data=[--, 366.0, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:subsample': masked_array(data=[--, 0.7314831276137047, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:k_nearest_neighbors:n_neighbors': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:k_nearest_neighbors:p': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:k_nearest_neighbors:weights': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:n_components': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:shrinkage': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:shrinkage_factor': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:C': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:dual': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:intercept_scaling': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:multi_class': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:penalty': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:C': masked_array(data=[--, --, 6.342897164595882, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:coef0': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:degree': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:gamma': masked_array(data=[--, --, 0.2229870623330047, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:kernel': masked_array(data=[--, --, 'rbf', --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:max_iter': masked_array(data=[--, --, -1.0, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:shrinking': masked_array(data=[--, --, 'False', --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:tol': masked_array(data=[--, --, 2.006345264381097e-05, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:multinomial_nb:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:multinomial_nb:fit_prior': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:C': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:average': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:qda:reg_param': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:random_forest:bootstrap': masked_array(data=['True', --, --, 'True', --, 'True'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:criterion': masked_array(data=['gini', --, --, 'gini', --, 'gini'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_depth': masked_array(data=['None', --, --, 'None', --, 'None'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_features': masked_array(data=[0.5, --, --, 0.9260795160807372, --,\n",
              "                    0.5240592829918601],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:max_leaf_nodes': masked_array(data=['None', --, --, 'None', --, 'None'],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:min_impurity_decrease': masked_array(data=[0.0, --, --, 0.0, --, 0.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_leaf': masked_array(data=[1.0, --, --, 17.0, --, 10.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_split': masked_array(data=[2.0, --, --, 7.0, --, 16.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_weight_fraction_leaf': masked_array(data=[0.0, --, --, 0.0, --, 0.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:n_estimators': masked_array(data=[100.0, --, --, 100.0, --, 100.0],\n",
              "              mask=[False,  True,  True, False,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:sgd:alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:average': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:epsilon': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:eta0': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:fit_intercept': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:l1_ratio': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:loss': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:penalty': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:power_t': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:tol': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:base_score': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:booster': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bylevel': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bytree': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:gamma': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:learning_rate': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_delta_step': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_depth': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:min_child_weight': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:n_estimators': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:normalize_type': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:rate_drop': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_alpha': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_lambda': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:sample_type': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:scale_pos_weight': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:subsample': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_imputation:strategy': masked_array(data=['mean', 'mean', 'most_frequent', 'mean', 'mean',\n",
              "                    'mean'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U13'),\n",
              " 'param_preprocessor:__choice__': masked_array(data=['no_preprocessing', 'no_preprocessing',\n",
              "                    'no_preprocessing', 'no_preprocessing',\n",
              "                    'no_preprocessing', 'no_preprocessing'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U16'),\n",
              " 'param_rescaling:__choice__': masked_array(data=['standardize', 'standardize', 'standardize', 'minmax',\n",
              "                    'robust_scaler', 'normalize'],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U13'),\n",
              " 'param_rescaling:quantile_transformer:n_quantiles': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_rescaling:quantile_transformer:output_distribution': masked_array(data=[--, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_rescaling:robust_scaler:q_max': masked_array(data=[--, --, --, --, 0.8245132980938538, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_rescaling:robust_scaler:q_min': masked_array(data=[--, --, --, --, 0.08947420373097192, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 1,\n",
              "   'classifier:random_forest:min_samples_split': 2,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.010000000000000004,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.051832615669195795,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 6,\n",
              "   'classifier:gradient_boosting:max_features': 0.8807456180216267,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 7,\n",
              "   'classifier:gradient_boosting:min_samples_split': 19,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 366,\n",
              "   'classifier:gradient_boosting:subsample': 0.7314831276137047,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'libsvm_svc',\n",
              "   'classifier:libsvm_svc:C': 6.342897164595882,\n",
              "   'classifier:libsvm_svc:gamma': 0.2229870623330047,\n",
              "   'classifier:libsvm_svc:kernel': 'rbf',\n",
              "   'classifier:libsvm_svc:max_iter': -1,\n",
              "   'classifier:libsvm_svc:shrinking': 'False',\n",
              "   'classifier:libsvm_svc:tol': 2.006345264381097e-05,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.9260795160807372,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 17,\n",
              "   'classifier:random_forest:min_samples_split': 7,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00034835629696198427,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gaussian_nb',\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8245132980938538,\n",
              "   'rescaling:robust_scaler:q_min': 0.08947420373097192},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00012586572428922356,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5240592829918601,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 10,\n",
              "   'classifier:random_forest:min_samples_split': 16,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'normalize'}],\n",
              " 'rank_test_scores': array([1, 3, 3, 3, 2, 3]),\n",
              " 'status': ['Success', 'Timeout', 'Timeout', 'Timeout', 'Success', 'Timeout']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhpbbHnV3b3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0nlmBEQR6Nd",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuHZbULXSTfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c072386e-d77c-4c66-bb01-2b0259e091be"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "\n",
        "# train model(s)\n",
        "knn_m = knn.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "knn_test_acc = knn_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(knn_test_acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7884996126366531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcFpZiIR87D",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJiVRVCXvKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca79ce50-77cd-49c8-ca03-73e35e7bc148"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "# Create a svm Classifier with PCA data\n",
        "svc = svm.SVC(C=1.0, gamma=0.1, kernel='rbf') # Linear Kernel\n",
        "\n",
        "# train model(s)\n",
        "svm_m = svc.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "svm_test_acc = svm_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(svm_test_acc))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.6978565894809331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WefgYsW4R_td",
        "colab_type": "text"
      },
      "source": [
        "# DT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmpigibbYm2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25e78fa6-3d6b-4402-c45f-148cf277710f"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "\n",
        "\n",
        "# train model(s)\n",
        "dt_m = dt.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "dt_test_acc = dt_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(dt_test_acc))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.794525264698287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zntQsupZSB_O",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N_gyiFQaL-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8538b4b2-9d83-4668-c3bd-f28c34a0e237"
      },
      "source": [
        "#Import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier with original data\n",
        "rf = RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_leaf=1, min_samples_split=5, max_features='log2', criterion='gini')\n",
        "\n",
        "# train model(s)\n",
        "rf_m = rf.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "rf_test_acc = rf_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(rf_test_acc))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7973659292416286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eToqzWdTSFFB",
        "colab_type": "text"
      },
      "source": [
        "# Neuron Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkdWo0y0h5PZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1302795-8541-4a89-825a-63670f98bb97"
      },
      "source": [
        "#Import svm model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create a NN Classifier with PCA data\n",
        "nn = MLPClassifier(max_iter=500)\n",
        "\n",
        "# train model(s)\n",
        "nn_m = nn.fit(PCA_X_train, PCA_y_train)\n",
        "\n",
        "# evaluate\n",
        "nn_test_acc = nn_m.score(PCA_X_test,PCA_y_test)\n",
        "print(\"Test Accuracy score {0}\".format(nn_test_acc))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy score 0.7011276577429629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z8eT3iobBY8",
        "colab_type": "text"
      },
      "source": [
        "# Comparision "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0yGI7PkbIpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "ee090e5d-33d7-463c-f37c-acd4f2873b20"
      },
      "source": [
        "from yellowbrick.classifier import ROCAUC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "classes = [\"Low\", \"High\"]\n",
        "# Instantiate the visualizer with the classification model\n",
        "visualizer = ROCAUC(dt,classes=classes)\n",
        "\n",
        "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
        "g = visualizer.poof()     "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFnCAYAAABU0WtaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8FHX++PHXzPb0Hkoo0jsK4imI\nKEVABTwrKqLIWTnPE8uph+IdFvDEinr61VO/nD/hxMZXT5BDLIcFiXQFpARCS2/bZ2fm98eSlZBA\ngpBsNryfjweP7M7MzryzCXnv+zOfopimaSKEEEKIFkGNdgBCCCGEOHEksQshhBAtiCR2IYQQogWR\nxC6EEEK0IJLYhRBCiBZEErsQQgjRglijHYAQAN27d6d9+/ZYLBYAdF1n0KBBzJgxg7i4OAAKCwt5\n6qmnyM3NxWKx4HA4mDhxIldddVXkPMFgkBdeeIGlS5dSPZJzzJgxTJs2DbvdXuu6x3p8NLz11lu8\n+OKLTJo0iVtvvfVXneO7775j6tSp5OTkYBgGmqZx1llncccdd5Cdnf2rzrl+/XqeffZZXnvttSMe\nM3fuXNq0aVPjZ9RQlZWVXHHFFQD4/X6Ki4vJyckBYPDgwTz00EO/Ku66rFu3jqeffpp9+/ZhGAZt\n27bljjvuYMCAAXz33XfMmDGDZcuWnbDrAVx33XXce++99O7dm7vvvpvvv/+eRx55hFdffTWyXYhf\nxRSiGejWrZu5f//+yPNAIGDedttt5lNPPWWapml6PB7z/PPPN5955hlT0zTTNE0zPz/fvPjii83n\nn38+8ro77rjDvOmmm8yKigrTNE2zrKzMvOmmm8zp06fXed1jPT4aJk+ebP7rX/86rnN8++235siR\nIyPP/X6/+fzzz5vDhg0zS0pKjjfERnd4/CfSjz/+aJ5xxhnmsmXLItv+85//mAMGDDC3bt3aqNeu\n1qNHD3PXrl2Neg1x8pCmeNEs2e12hg4dyk8//QTA+++/T1paGnfccQdWa7ihKScnh9mzZ/Pqq69S\nVVXFzz//zBdffMGcOXNISkoCICUlhccee4zLLrus1jUacvy1117Lhx9+GHnNoc+7d+/Oyy+/zOjR\no5kzZw6zZs2KHFdaWsqpp55KVVUV27ZtY9KkSYwePZpx48axYcMGADweD9OmTWPs2LGMGDGCGTNm\noGlajRifeOIJ1q5dy7PPPsvzzz9PIBDgoYceYvTo0YwdO5bZs2ej6zoAw4cPZ968eYwePZp9+/Yd\n9f11OBz8/ve/5/TTT+eNN94A4MCBA9xyyy2MHj2a0aNH88UXX0SO/+CDDyLb77nnHoLBIN999x2j\nRo0CYOvWrVx55ZVceOGFnH/++fzzn/8E4L777uPFF18EYPPmzUycOJExY8YwYcIEvvrqKyDcmnDl\nlVcyd+5cxo4dy/Dhw1m1atVR4wfYs2cPZ599No899hiTJk0CIDc3l0svvZRRo0ZxxRVXkJ+fD4Bp\nmpH35rzzzuORRx6JvG8vvfQSV155JSNHjoyce8SIEcybN4/09PQa1/T5fPzxj39k9OjRDB8+nDlz\n5kT2ffLJJ1x00UWMHTuWcePG8d133x11+/Dhw1m9ejXXXnsthmEwdepUvvjii8h2gP/85z+MGzeO\nESNGcMMNN1BaWgrA888/z4wZM7jssssiPz8hqkliF81SRUUFH330EaeddhoAq1at4rzzzqt1XPfu\n3UlLS2P9+vWsWrWKU089lZSUlBrHpKenc9ZZZ9V67bEeXxfTNFm6dCljx45lxYoVke0rVqzgzDPP\nJD4+nmnTpjFhwgSWLl3Kww8/zG233UYoFOKDDz4gKSmJTz75hKVLl2KxWNi2bVuN8997773069eP\ne+65h9tvv50333yTAwcO8PHHH/P++++zevVqPvroo8jxBQUFLF26lDZt2jQo/uHDh0cSzZ/+9Cd6\n9OjB0qVLeeWVV7j33nspKytjz549zJkzh//93/9lyZIl+Hw+/vd//7fGeebNm8fEiRP5+OOPWbBg\nAV9//TXBYDCy3zAMpk+fzqRJk1iyZAmPPPIId911F263G4Aff/yR/v3788knn3D11Vfz0ksvNSj+\n8vJyevbsyT//+U/cbje33nor06dPZ9myZUyePJk77rgDgA8//JAlS5awaNEili1bRn5+Pm+//TYA\n33//PcOGDat17rPOOou0tLQa295++208Hg9Llizh/fff57333osk4b/85S+8/PLLfPLJJ8ycOZPP\nPvvsqNurzZ8/P/L10Djy8/O59957mTt3LsuXL+c3v/kNDz/8cGT/F198wSuvvML111/foPdKnDwk\nsYtm49prr2XMmDGMGDGCESNGcOaZZ3LjjTcC4USfmppa5+syMjKoqKigoqKiVoV1NMd6fF3OPfdc\nAPr164dpmmzevBmAZcuWMXbsWHbs2EFJSUmkBWDgwIGkpaWxZs2ayNf//ve/GIbBX/7yF3r27HnU\n633++edcccUVWK1WnE4n48aNY+XKlbXiaaiEhASqqqrwer189913kSTRoUMHBg4cyBdffMHKlSs5\n7bTTyM7ORlEU5s6dWyuZpKens3TpUjZt2kRqaiovvvhijT4Ke/bsobi4mAsvvBCAvn370qZNm0jr\nRXx8fKRi7t27d70tDtU0TYu0GuTm5pKdnc2QIUMAuOiii9i9ezf79u1jxYoVXHrppSQmJmK1Wrn8\n8sv59NNPgfDvQUZGRoOud8MNN/Diiy+iKArJycl07dqVPXv2RN6DBQsWsHfvXk4//XTuv//+o26v\nz5dffskZZ5xBt27dAJg4cSKfffZZpKWhf//+tT54CAHSeU40I/Pnz6dVq1aUlpYyZswYLrjggkiz\ne2pqKoWFhXW+rri4mLS0NCoqKigoKGjw9VJTU4/p+LocWu2ff/75LF++nPbt2/PDDz/w5JNPsnXr\nVvx+P2PHjo0c53a7KS8vZ+zYsVRUVPDss8+yY8cOxo8fz/3333/UTnulpaUkJydHnicnJ1NSUlLj\n+bHYu3cv6enpVFVVYZomEydOjOzzer2ceeaZeL3eyK0KCDfjH+7uu+/m5Zdf5o9//COBQICbb76Z\na665pkbciYmJKIoS2ZaUlERpaSkZGRkkJiZGtquqimEYDYrfYrGQkJAAhDvb5efnM2bMmMh+u91O\naWkpVVVVvPbaayxcuBAId86sTorVvwcdOnSo93p5eXnMnj2bHTt2oKoqBw4c4JJLLgHCTfovvfQS\nl1xyCa1bt+aBBx7gjDPOOOL2+lRVVbF69eoa309CQgLl5eXAsf+sxclDErtodtLS0rj22mv529/+\nFmmSPeecc5g/fz7Tpk2rcezWrVupqKigX79+ZGZm8vjjj1NQUFCjp3dlZSWvv/46f/jDH2okljPO\nOKPe4w9PMhUVFUeMe/To0Tz66KN07dqVQYMGkZCQQFZWFvHx8SxZsqTO10ycOJGJEydSUFDA7bff\nzgcffBDpCV6XjIyMyB92CDdFN7TarMvSpUsZMmQI6enpWCwW3n33XeLj42scs3DhQtasWRN57na7\n8fv9NY6Jj49n+vTpTJ8+nfXr13PjjTcyePDgyP709HQqKiowTTPyMygvLz/uFpNDZWVl0alTJ957\n77069w0fPjxyL/5Qv/nNb/j0009rJdt33303Ui1X++tf/0rv3r154YUXsFgsNT4ItW/fnscffxzD\nMPjggw+46667+Oqrr464vSHfz+DBg3nuueca+hYIAUhTvGimpkyZwpo1ayKdqMaPH08oFGL27NmR\nDmb79u3jvvvu47bbbiMuLo7OnTtzwQUXMH36dIqLi4Fw8pg+fTplZWU1kjrQoOMzMzMjzetr1qwh\nLy/viDGfdtpplJSU8N5770Uq9LZt29KqVatIYi8tLWX69Ol4vV5eeOEFFi1aBEB2djY5OTm1Yjzc\nueeey6JFi9B1Ha/Xy4cffljn/eH6BINBnnnmGfbs2cM111yD1Wpl2LBhLFiwAAh3Erv//vvZv38/\nw4YN44cffmDPnj2YpsnMmTMjcVe75ZZb+PnnnwHo1q0bCQkJNb6XnJwcWrVqxb///W8AfvjhB4qL\ni+nXr98xx34k/fv3p6ioiHXr1gHhe9T33HMPpmkyYsQIPvzwQ3w+HwALFizg/fffB+DWW29l8eLF\nkecQvpUyd+7cSGtAtZKSEnr27InFYmHlypXs2rULr9dLaWkpU6ZMwe12o6oq/fv3R1GUI25viLPP\nPpvVq1dHOgCuX7+eRx555LjfJ9HyScUumqWEhARuuukm5syZw6JFi7BYLLz++us8+eSTjB07FqvV\nisPhYNKkSVx++eWR182aNYuXXnqJa665BkVRsNlsjB8/nqlTp9Z5nfqOnzJlCtOnT4/c76y+f1sX\nRVEYOXIk77zzDnPnzo1se+qpp3j44Yd55plnUFWVKVOmEBcXx4QJE7j//vv5n//5HxRFoX///kyY\nMOGo78u1115Lfn4+F154IYqiMGbMmBrN/Eezf/9+xowZg2maeDwezjrrLN56661IM/jDDz/MzJkz\neeedd4Dwh6nWrVsD4Ur1uuuuw2Kx0LdvX6ZMmcLatWsj5540aRJ33XVX5EPX1VdfTceOHWu8N089\n9RQzZ85k3rx5uFwunn322cgcBSeC0+nkueeeY9asWXg8Hmw2G3fccUfk5/Lzzz/z29/+FghX148+\n+igAXbt25R//+Adz585l3rx52O12OnTowBtvvMEpp5xS4xbQrbfeyuOPP86LL77IiBEj+P3vf89z\nzz1Hz549GTp0KJdeeikWiwWbzcajjz5KWlpandsbIisri1mzZjFt2jQ0TSM+Pp4HHnjghL1fouVS\nTFPWYxdCCCFaCmmKF0IIIVqQRk3sW7duZeTIkZHJKg719ddfc9lll3HllVfywgsvNGYYQgghxEmj\n0RK71+tl1qxZR5zo45FHHuH555/n7bffZuXKlbUm5hBCCCHEsWu0xG632/mf//kfsrKyau3Lz88n\nOTmZ1q1bo6oqw4YN45tvvmmsUIQQQoiTRqP1irdarZHJRQ5XVFRUY8aktLS0yJCOuhiGEenl2tCh\nIkIIIUQsM00zMiJCVRteh8fEcDePx8PWrVujHYYQQgjR5Lp161Zjdsb6RCWxZ2VlRSYEgfDCFXU1\n2Vez2WxA+JtrLmtkt0QbNmwgPcfBuvzP8AQrcFoT6NtuGO3TektLyQm0ceNG+vTpE+0wWjR5j38d\n0zQxTJ1AyEcg5EFBQcGCQYhKbzGaHiDBmYZhhNiWt4X4VAuqYiXekYJhhqjyleIOlJIUl4mKBd3Q\nKKzahWkaJDozCekagZAPn1aBqjjC5zZ1dNMHGJhAU/ylqR7k7dEshHQVw4QkRwh30EK534amKyQ5\nNJw2kxKvE6/mAEUh0+UHxUK5PxlVteCwmsTbAphmHIaSjN1ixWEJYbUY2KzpOK1OHDYrNiVEnMNJ\ngiOJeLuDOLudOJsNp1XFYvmlEq+efjoQCPDJJ5/gcrnYunVrJAc2VFQSe05ODm63mz179tCqVStW\nrFjBk08+ecTjq5OK3W6vc55qcfwqvEXkBb9i47YCFEWlV5uz6d9uOHarM9qhtUjye9z4Yu09Nkwd\n3dDRDQ1vsBLd0HBY49CNEJ5ABZW+YlTVgsuWgG6EKPcV4PaXkRbfFjAJ6n72l4c7IafGt0Y3NHzB\nKsq9BcQ7UrBZHOhGiDLPAUJGEKctATAJGRrBkI9wSj3GaU2K6t58wF27M3RlMDzRT8gAiwJBXcGr\nWdB0hXi7jlU1KXDbqQpa0XSFdsl+vJqFXeUuNF0hwR4i1RWiwG3ngNuFqljIStCIs5kccKdgUe04\nLAopLg1TcRIyknFY7bisCk67isOagMvmwmWzhROr3Uq2zUKc3UqczUqc3XLwa/ix02ppsoJG13Us\nFgsOh4M//elPxMfHk5WVRSAQADjmOBotsW/cuJE5c+awd+9erFYrS5cuZfjw4eTk5DBq1Cgefvhh\n7rrrLgAuuOACTjnllMYKRRyFFgqwLv8zftz3XwxTp01KV87oNI6UuCO3oAjR0pimgYmJbuj4g268\nwUoURcGiWtGNEBW+InzBKlLjWmGYOn7NQ0FlHoqikuzKRDc03P4yyr0FJMdlYbXY0Q2NAxU70Q2N\nZFcmhqmj6UGqfMVYLY5wojU1ApoXw9R/dew7itbW2lZUtbvGc2+wMvLYMMMp/ECVB7+mohkKmXEK\nHs1Cocd+MInqJDhCFFQ5KPDYCRkqrRL8WFX4qSiekKFgVUyyEoKU+63klbsI6Qoum06yA8r9cZiK\nHatqI8kBFtUBiguHzXFI8rQSdzCxWlUrqt1Ch+ya2+MOSbzxBxOuy2bFZmkZU7CYpslTTz3FkiVL\n+Pe//43NZqt39smGaLTE3qdPn8g6w3UZNGhQZKUl0fRM02RH0VpW5/0bX7CKeEcK6WYvzus9Tprd\nRZMyTAPdCGEYIXQjhDdYSVD3E2dPQjdC+IJuSj37UBWVOEcyuqFR6SuhyldManwrVNVCSNfYU7YF\nw9RJj29DUbCQko1rKfPsx2VPjFS+Fb4iND2A3erENE10M0RID9Yf5DEo8eyttc2vuTFNBQMVFZ2g\nbhDUdUKGilVVsagm5X4nFX47AR0y43zohsm20jg0XcVp1cmI0yjx2SIVbIpTI8Gus73UhS9kQTcU\nMuKC+EIWdleEjzFME5sKnqAFzbTgsNqIs9kOqU7DX12HJVFMC4ZuJT3FQvssKy7bLwl3ZO/wMbu2\nb2Vgv741ErDTakFV5e9HQymKws6dO9m3bx87d+6stejQrxUTnefEiVXq3sd3OxZTUJmHRbXSv90I\n+uYMY93aDZLUTzKGaWAYIRRFRTdC+DU3bn8ZJmCz2NGNEFX+UjyBclLislEUCIb87CvfhmkapMa3\niiTjUs9+Ep1pOKwudCNEUeVugrqfBEcqKKDrGpX+EiyqDbs13CwcDPmPq1rNK9lQa1uZZz8AFQcX\nwfNp7oN7qn+3TfyaiYEdw3SiUoVhqvhCSWiGikIQm+rHHXRR7I0jEIJ4mxebqrGlJAFPUCGk67RK\nDOIOWthWEodmqNgtBgn2EEUeO+6gFc1QsCgG/pAFd8CCcZTRxU6rpWZTsM1CvP2XhGtYrBQHrcS5\nLAxIqlnVDq9O0Icl5zh7+BzV5zzRVa6rLJ8e2bJ07LEKhUJ88cUXjBgxAoDHHnsMwzBqLAF9vCSx\nxzDTNFm/ZwU/7v0vhtmw9asBND0AmLRP68WgTheR6Eyr9zXixDNNg5AeQje1g/dXwwkyoHlJcKSG\nOzFpXorc+WAaJDjTDt5vLafMW0BKXCY2ixPd0Nhb9jOaHiA1Pjs8REYPUOLei8Mah8ueGE7QgVKC\nIR9W1Xaw2TmEeQy/N3XZVbKxxvPqpHqowMFrqmq4oxSGgm44ASeqqmIaASxqIoqShG6qhPRyTFMj\nYGSj6QohI4RVqcIXslPsTcEXAsX0Y1P97K+Ko9yv4gmauKw+qgKws8yBZqjohoLVYuANWtAMFcM8\n0ofW1kf8/lRFiTQBH34ftjgQTqKntqu9/9CkXOc93EOaml02q1S5J5Hbb7+dhQsX8uGHHzJ06FCS\nkpJO+DUkscco0zRYteMjftr/NQ5rPAnO1Aa/1m5x0q/debRNPTHNPrHINA10Uyekawfvo2oENR8V\n/iIMw8Rhc6IbIdyBMty+MpJcGVgs4fut+aU/oesaaQltMIwQ/pCXYvce4u3JkebjEvde/CEPLlsi\niqJEErKiqCiGhS3ffkRIDxzTB7LD5ZfW3lbhK6zxPBDyUukvRlWsgAmmic3iwGlLwKJaqfKHT5Ia\n1xoTFU0P4g1WYFUTsNuyCRkqfq0STfdgmJkETRtaCAyjlIBuoTyQgS8I/pCGgo8yn41KvwW3ZuLx\nh6gImJT7wKPp+LSGVuatDntuA1yHbXPisMbVSpTxdiv92lrQvG5aZ2YcIZlacdWRqA9NytXH2yyq\ntGKJE+rGG29EUZRGHbUhiT0GGYbOf39exI6iNaTEZXN+n6nE2U/8p77GZJg6WshPSNcwMDAMHW+w\nEp/mDjfdAlrIz4HKHeiGTkpc5sHewm5K3XtJdKXjtMWjGyEOVOwgoHlJdKWjEE6iJZ692FQH8c7k\nSK/igOZBVSyYmMfV/Fttb3nNuRUqvIW1jgnpQawWOxb14H8100RVrMTZw7EHNC/xjiRS4sL3it3+\nMgIhD1mJHbFZ7JiYlHoKsahOklwd0QwFX9CHN1iOSTIh00UgBH7NQ1BX8GgJeIMWPEEDXyiIOwAe\nzcQb1PFpIbxBHW+Nr2l4NR1Nr/6AYQWql1L1HvyqAomA/+A/CCdbgMM/XYRQFGo0AWcl/pIoXbWq\n118Sa7zdiqsBFW7cweMsR5mwIzc3l4EDBzbgpyhE49q6dSsPPfQQzz33HFlZWQwYMIAXX3yxUa8p\niT3GhHSNL7b8P/JLfyIzsT0je12Pw3bsa1pXj1fVD3ZY0o0QAaOKcm8hLnsifs1NuaeQYnc+8Y7U\ng/dbNfaWbSUY8pF8sNe8pgcoqtqN0xofaSou9x7AG6zEYY1DVSzoZghfsAoTM3L/NaRrmBxHM3B5\n7U2V/uIaz4P48AYrsKg2ODhC1mGLI86RjEWx4gmUo+lBMhJzsKhWDNOgyleC0xZPq+ROWFQrvoP3\nnNPjW+O0J2JRrVT4irGoVrISO6IbVnyhEBW+CgzTQch04tcUfJqBTwNfSKFSM/AFQniDIbyaTt6e\nfSSkptVIsuGkG8KrOfEEqx/78Wk6pmkHDGDHYd+xl2Nlt6g17uFmxjtr9DY+UoV71P0Hk3L1c7tU\nuUJErFixgk8//ZT333+fm2++uUmuKYk9hgRDfpb/+CYFlTtpk9KV83pei0W14g1WoRtBQrpGyNAo\n9xaws2g9Qd0X6aAU0gNkJOSQk9YD3QixvfAHyrwHal1j6w9LGhTL/ortNZ5XUkxh1a4a2wzTwG5x\nYlGt4UrZNHBY48JN32Z4WJHTlhBJrJ5ABd5gBRnxOThs8aiKhTLvfqyqnZy0HpGhR5W+YhKdaeEE\nrVoJhvxYFCtJrnTs1vD4Vm8wSNBQ8GsmXk0PJ8qDidUbDOHWQngNHV8oxJ4CPZxMtZrHeLUQvqAT\nr5aJNxjAq3lq7A/qte8nN1zNSldRqFGlZiY4D6lO665wa3awOnqFG2cLV7nWFjJMSIjmLC8vj/bt\n26OqKjfeeCM9e/bknHPOabLrS2KPEX7Nzacb/kGpdx9tUroyotd1WFQry398k/zSn+p8japYIk3A\nmh5gb/nWWs3HrZO74LC5wITdJT/htMeRntAWpy0eBRVvsIq0+FYkOFOxqDbcgTJUFDKTOuCwxqEo\nKt5ABQ5bHA5rPBbVGvmnKrUneNANA18kcYa/eg4+DqkhQpYQOyr1g0k1hFdrHT52S+hgcg3hDdrx\nauX4gsV4Nf2QCveXJmbzGOfZOBqbRa2RINPjHZGmZtdRKtwj7d+1bSsD+/c95F6uFYdVqlwhWoKl\nS5cyZcoUHnzwQW699VZUVW3SpA6S2JsNw9Ap8x7AG6jAG6zCG6zEG6zEEyjHZUtkV+kmQnp4FqIS\n914+XPMM3mBlZAxuojOdtqldsah23IEynNY4zux8MYqiUOUvobByN4qioioKqmLBYY3DYYsj0ZmO\nRbVimibffr+aHn361ahYFZtOWTDEXk910kzEF9Txaj48wapDEnTRYc3Kep0VcCB0fL2wD3dolZoe\n56Bdyi/V6eHNxTUTb/0VbmNNhuEszad7lgwTEqIlGjBgAB06dKBdu3ZRi0ESezMR1P3839rnj3qM\nRbWhYCEQ8oanZVSTcdjiMIjDo2ezvrDTwWq4HV4txKJNuYd1lApFqmXPYRWuN6hjmCYs3HxCvh+L\nqhzSgcpCqisuMq7WdZQK95d7uUceKlR9T7gpp3wUQoi6GIbBK6+8wqBBgxg4cCCZmZmsXLnymFZj\nO9EksR+H8Hhho0ZFWmelekjnKE9ku0aSbTtF3iRKfTYSbcX0ykzAq1lYdyCTYo+FLmml/KZdMXaL\nyVd5Kbyxpg1HXiKhiCNO3HwYl81ySMK10zY5jjibhZDfS3Z66pF7JTus9Va4jTUZhhBCNEdr167l\ngQceYPDgwXz00UcAUU3qcJIkdm8wxK4yzxGTr6+O6rWu/Z5g7WN041hv5pr0yvRwVb/9tEkK0jZB\nxWE1qJ6foshj55v8ZOJsVtxaCh1SQrRPKcOt9+HSfol1Vrh1TQlZq8JtwJSPMkRICCHqZxgGgUAA\nl8vFgAEDmDdvHqNGjYp2WBEtPrF/vbOQS9/4gkK3v/6Dj+LQKR+TnXZaJ/4ynrbuClchwa7hsifj\nslmxK9sxQpsAPyG9InLeOBukJ55C6+ROtEnpRGZie+4ZHR4jrOlB3v42lyRXNguvu+i44hdCCHH8\niouLmTJlCh06dGDevHkAXH311VGOqqYWndjnr97BTf/6Bt00ufb0TqTHOWok3/omw6iudJ31TIbh\n1zzsLFqHboSAEMGQj22FuQRCPq4dPAu3v4yvti6nIFCIRbXSKfNU2qf1BsWkbUp3bNa6l5fcV7YV\nwwzRPr1XI71DQgghjkVycjJut5uKigo0TTvmtdKbQotM7IZh8uCStcxevpFkp42Fk89hVPc2jXa9\nLfu/Zc3uZTW22SwO2qX15Ie8pWzc+yWGqeOwxjP+tNuJdzRssv/dpT8C0D5NErsQQkTL7t272bJl\nC6NGjcJms/HBBx+QlJTUbDvvtrjE7g5oTP5/K/lwYz5dMhL58IbzGn0For7tzsVudZJwcDEVhfCY\n5FU7/o+84vXEO1I4rf0oTsns/8vUovUwTJ09pZtx2RNJT2jbmOELIYQ4gkAgwNixY3G73axevZrM\nzEySk5v3cNUWldjzyzxc/I8VrN1XxnldsvnXdcNIi6u7mft4GKZObt5SHNY4+rU7F1Wx0LPNkMj+\nkKGx6Ps5+DUP7dJ6YpgmecUbyCuuvcTkkYSMIIGQl26tfoOiSA9zIYRoSqZpoigKDoeDmTNnApCR\nkRHlqBqmxST2NXtKuejVzzhQ5eN3Z3Zh3iW/aZQhV5oe4LOf5rO/fBspcVn0bnt2rSrcqto4u+tl\nVHiLyN215FcvOKIqFjpnnXaL6/GFAAAgAElEQVQiwhZCCNEApmny5ptv8sEHH7Bo0SKsVitXXHFF\ntMM6Ji0msd/7f7kcqPLx9ITTuX1oj0a795FXtJ795dtom9qNYd2vrpHUTdNEN0NYVRsOW9zB++4K\n5/eZSmZC+2O+lqKqWNXm1zFDCCFaKkVRWLVqFWvXrmXLli307t072iEdsxaR2L3BEP/dWciAnDT+\ncE7PRr2WP+QBoGfrIditzsh2T6CCb7a9R7m3iD45Q1mzaxm6EeK8npNok9K1UWMSQgjx65mmyerV\nqxk0aBAAjz32GDNmzKBNm8brdN2YWsTN2//uLCSoG4zo2rrRr1VQkQeAy54AhIe6/bDrU97/YS57\nyrYQ50hi7e7lBEJehnS9VIaqCSFEM/enP/2J0aNHs3LlSgBSUlJiNqlDC6nYl28NL585omurRr1O\nQPNS7i0gO+kUUuNa8932xfxc8D0hQ8NpS+DUdqPYsv9b/JqbQadcSJdsmcVNCCGau8svv5xdu3bR\noUOHaIdyQrSIin35zwdwWFXO7pTVqNexWuyM7D2Fc7pPRFVV3IEy7NY4Bp1yEeNP/QM7itZQFSih\nb8659G47tFFjEUII8evs27ePm2++meLiYgAGDRrEwoULycnJiXJkJ0bMV+zFbj9r9pYyvEsrXLbj\n/3Y0PcD6/BVoes0paE3TpMS9F59WRU5quHOew+qiXVoPqvzFLP/pTUo9++iWfQYDOow+7jiEEEI0\njsWLF/POO+/QvXt3pk+fHu1wTriYT+yfbTsAwIhuJ6YZPq9oPRv2fH7UY7Yc+K7O7R0z+nJml4ub\n7WxEQghxsioqKiI9PR1VVbnxxhtp06YN48aNi3ZYjSLmE/vyn6vvr5+YjnPF7j0AnNdjEkmuTAB+\n2vdfthZ8D8CgU8bRJqVLrdepqkqSM0OSuhBCNDNfffUVkydP5s9//jO/+93vsFgsjB8/PtphNZrY\nT+xbD5DisjMgJ+2EnK/YvQdVsZCT1gOLasXtL2drwWoAOmWeSu+2Q+o5gxBCiOakW7duJCUl4XCc\n+JlIm6OYTuw7SqrYWermt33bH3X1tYbSjRBlngOkxbfBF3Tz0/6V/LTva8DEYY3jzM4XH3/QQggh\nGpVpmrz77rt06dKFU089lezsbL7//nvsdnu0Q2sSMd0r/j/Vw9xO0P31Ms9+DFMnI7Etmh5g096v\nUBUVi2pleM/JNSakEUII0Txt3LiRm266iXvvvRfTNAFOmqQOMV6xVyf2kSfg/rppmmzY8yUAafFt\ncVhdnN97KtnJp2CYBjbLyfNLIYQQscY0TUKhEDabjb59+/L4448zevTok7LfU8xW7LphsGLbAdqn\nxtMlI/G4z/f1tnfZVRJefW3rge/4aN08spI6YFGtktSFEKIZq6io4LrrruOuu+6KbLv55pvp2LFj\n9IKKophN7Gv3llHqDTKia6vj/kRW4Svi54LVqFiAcAe6tPjWBA8byy6EEKL5cblc5OXlsX37dvx+\n+bsds03xJ3KYW17RegAMwsur9mw9mDM6jTspm3CEECIWFBcXs2XLFoYMGYLdbmfRokVkZGSgnoCO\n1LEuZhN79f314Sdgfvj95dsjjxOd6QzqdJEkdSGEaKY0TeP888+nrKyMb7/9luzsbLKyGndK8VgS\nk4ndp4WXae3XOpXsRNdxnUs3QhRW7Yo875dzHqoin/iEEKK5stlsTJ8+nYqKCjIyMqIdTrMTkxls\n7d4yAiGDYV2yj/tchqkzsOMYUlzhc2Ult4zVfYQQoiX56KOPuOaaawiFQgBMmjSJadOmYbFYohxZ\n8xOTif2nggoAerdKOa7zbN7/DZoeoHfboRjo2CwOkpzpJyJEIYQQJ9DixYv57LPPWLduXbRDafZi\nMrFvLgwn9p5Zyb/6HGt2LePb7R/yyfqXCWg+Kn3FZCTkoEgzvBBCNAubN2+OPJ49ezaff/45AwcO\njGJEsSEms1h1xd4z+9cl9jW7lrEufzmqYqHKX0JecbhXfHpCy1iLVwghYt0jjzzCkCFD+PbbbwFI\nS0uje/fuUY4qNsRkYt9cWEFmgoP0+GOb0N80TX7YtZR1+ctx2hIwTJ1OmadGxqtnJEpiF0KI5mDk\nyJEMGDCA1NTUaIcSc2Iusfs1nZ2l7l/VDL9p71esz19BojMNpy0BUOjXbjglB5dqTU9oe4KjFUII\n0RAVFRXcd999lJaWAnDmmWfy6aefSpX+K8RcYt9aVIlpQo9f0QyfHJdFSlw2o/vcSLn3AFlJ7UmJ\ny6K4ag8OazwJDvlkKIQQ0bBgwQJeeeUVXnrppcg2mU/k14m5cezV99d7/IqKvV1aD3JSu+ENVgEQ\n70jBr3lwB8pom9pNfomEEKIJVVZWkpiYiKIo/O53v8PlcnHVVVdFO6yYF3MVe3WP+GNJ7HnFG/Br\nHgAURcU0TTqk9yYrsQPFkWZ4ub8uhBBNJTc3l8GDB/PGG28AYLFYmDx5MjabLbqBtQAxW7E3pEe8\naRrkFW/gyy0LSHJlMrjLJZGqvHfbcwDYXrgGgAxJ7EII0WTatGmDpmlUVVVFO5QWJ+YS+5bCSuLt\nVtqlxNV77M8FuXy97V0AKnyFfLLh70c8VhK7EEI0rs8++4ysrCz69OlD69atyc3NJSEhIdphtTgx\nldh1w2BLUQV9WqU06H74vvKfAYizJ5ORmMPukk0AxNuTOSWzf2QympS4LOIcSY0XuBBCnOQ2b97M\nZZddxmmnncZ//vMfFEWRpN5IYiqx76nwEQgZDe4RX+Y5AEBWUkc8gTIAhnS9jJzUHrjs8gslhBCN\nzTAMVFWlR48ezJgxg5EjR0pH5UYWU4l9e3H4XkxDO855AuUA5BWH5xa2WZx0zOiHzWJvnACFEEIA\n4PV6eeihhwB48sknAZg+fXo0QzppxFhirwQalth1I4SqWsEIcmbnCWQmdiDekSRJXQghmoDFYuGb\nb75BURS8Xi9xcfX3ixInRmwl9pJwxV5fj3hfsIp3vp+NzeLEYY2jR+uzmiI8IYQ4qbndbrZs2cLA\ngQNxOBwsWLCArKwsHI5jm/5bHJ/YSuzFbqyqQpeMxDr3G4bO9zs/5qf9XwMQMoLE23/9CnBCCCEa\nRtd1xowZw759+/jmm2/Izs6mXbt20Q7rpNSoif2xxx5j3bp1KIrCAw88QL9+/SL73nrrLRYvXoyq\nqvTp04c///nP9Z5vW0kVXTISsVnqnlenyJ3PT/u/xmVLxG51UeErxGGT5h8hhGhsFouFG264gfz8\nfJKTpaCKpkabeW7VqlXs2rWLhQsX8uijj/Loo49G9rndbl577TXeeust3n77bbZv387atWvrPWel\nXztqj3i3P9zzvVPWaVT4CgFw2uKP8zsRQghRl6+//ppbbrkFXdcBuOGGG5g5cyZOpzPKkZ3cGi2x\nf/PNN4wcORKAzp07U1FRgdvtBsBms2Gz2fB6vYRCIXw+X4M/4R2t41x1L3jvwa8ADqtU7EII0Rhe\ne+01Fi1axHfffRftUMQhGi2xFxcX11hHNy0tjaKiIgAcDgfTpk1j5MiRnHfeefTv359TTjmlQec9\nWmLvkzPs4MpthZHJZxxSsQshxAmzZ8+eyOMnnniCTz75hMGDB0cxInG4Jus8Z5pm5LHb7ebll19m\nyZIlJCQkcN1117F582Z69OhR/4lK95GbW3bE3Zrpo8x/AKeSjJ8KSgrKyC3NPRHfwkkhN1feq6Yg\n73Pjk/f4xHvnnXd47bXXeOqpp+jVqxd5eXmoqirvdTPTaIk9KyuL4uLiyPPCwkIyMzMB2L59O+3a\ntSMtLQ2A008/nY0bNzYosf922G9IcNRe/afSVwwoHKjYAdugVXoH8orX06ljV7q1GnhivqkWLjc3\nl4ED5b1qbPI+Nz55jxtHIBBgxYoV9OrVC0De40YWCATYuHHjMb+u0ZrihwwZwtKlSwHYtGkTWVlZ\nkXmB27Zty/bt2/H7/QBs3LiRjh071nvO1kmuOpM6wIY9n/Ne7t8I6QEyEtuR4AjfBpCmeCGE+HV8\nPh+zZ8+mvDzcb2nw4MGsXLlSEnoz12gV+4ABA+jduzcTJ05EURRmzpzJe++9R2JiIqNGjWLq1KlM\nnjwZi8XCaaedxumnn17vOTun1z1+HeBAxU7sFic92gymV9uz+SEv/KHCaZXELoQQv8Zbb73FE088\ngcfjYdasWQBYrTE1/clJqVF/QnfffXeN54c2tU+cOJGJEyce0/k6H2FimqKqfKr8JeSk9kA92GnO\nH/IAyDh2IYQ4BoFAALvdjqIoXH/99Xi9XqZOnRrtsMQxaLSm+MZwpIp95c+LACiozGN9/goAAlo4\nsTslsQshRIP8+OOPDBs2jPnz5wPh6vwPf/gD8fHS8hlLYiqx1zWVrF9zU+4tINGZgab7sajWg9u9\ngILd6mriKIUQIjYlJydTUFBAXl5etEMRxyGmbpZ0y6yd2N3+Mly2RIIhH1bVRtfsQQAEQl4cVheq\nYmnqMIUQImasWbMGl8tFjx49aNu2LatXryY9PT3aYYnjEFMVe2pc7RWCMhLbMeG0PxIIechK6ojd\nGp7K0K95ZNY5IYQ4ih07dnD++edz2223YRgGgCT1FiCmKvYjKfcVAJAW3wYIT4YTCHlJdKZFMywh\nhGjWOnXqxB133MHQoUNR1Ziq88RRxHRiL6zczaa9X5DkCk98kxqfDYCmBzBNQxaAEUKIQwSDQebO\nnUtVVRWPPfYYADNmzIhyVOJEi+mPaPvLf2ZXySbc/jIGdhxLVlIHINwMD7IAjBBCHMowDBYvXszH\nH39MVVVVtMMRjSSmK/ZKfwkAp3UYRZIrI7I9EPICMuucEEJomsbPP/9Mr169cDqdzJ8/n6ysLBIT\njzzhl4htMV2xV/lLUVCId6TU2C5j2IUQIlyhjxs3jgkTJkRW1+zSpQtJSUlRjkw0phhP7CW47Im8\nl/sk6/M/j2yXpnghhABVVbn44osZO3YsDkftUUWiZYrZpnjTNPFrHhIcKVT5S7Gqv3wr0hQvhDhZ\n/fjjj7z++uvMmTMHVVW5+eabURQl2mGJJhSzFbtpGrRN6RqZgKZ1StfIvoAWTuyyAIwQ4mQzd+5c\nXnvtNVasCE+vLUn95BOziV1VLYzodT2BkA+XLZGUuKzIPlkARghxMiktLY08fvzxx1mwYAEjRoyI\nYkQimmI2sQNU+Irwa25ap3Su8ak0UrFLYhdCtHBvvvkm/fr144cffgAgKyuL888/P8pRiWiK2cTu\nCZRH1lzPTGxfY1+485wsACOEaPk6depEUlKSjEsXETGb2Es9B9hduonspI60TulSY18g5MVudcoC\nMEKIFkfXdV5++WUqKioAGDp0KLm5uQwbNizKkYnmImYTe37pjwD0bzeixv11CDfFS8c5IURLNH/+\nfO6//34ef/zxyDaXS1onxS9idrhbqXs/iqLSKrlTje2maeIPeUhwpkYpMiGEOLF0XUdVVRRF4Zpr\nrmHPnj3cdttt0Q5LNFMxWbGbpkmZ9wAKCvmlP9XYJwvACCFakry8PC644ALefvttAGw2GzNmzCAt\nTVavFHWLycReULkT3dAwTB2rxV5jXyAks84JIVoOi8XCTz/9xKpVq6IdiogRMdkUX+EtwqJaMU3I\nTj6lxj6/JrPOCSFi244dOzAMgy5dutCuXTtWrlxJu3btoh2WiBExWbHnpHVHN0K0SemMVbXV2CcL\nwAghYll+fj5Dhw7l5ptvRtd1AEnq4pjEZMVeUJkHQKvkzrX2ReaJl6Z4IUQMateuHZMnT+aMM87A\nYpEhu+LYxWRi37L/OwAyE2t/io2s7CZN8UKIGGAYBq+99hoFBQXMmDEDoMZQNiGOVcw1xZumSWHl\nLpy2BNIS2tTaLwvACCFiSTAY5NVXX+XNN9+kvLw82uGIFiDmKnZ3oBQTg1bJnbBZaq8v7I8s2SpN\n8UKI5skwDHbv3k3Hjh1xOp28/vrrpKenk5KSEu3QRAsQcxV7UWU+UHczPEjnOSFE82aaJldffTWj\nR4+mpKQEgF69epGdnR3lyERLEXMV+66SjQCYZt37PYEKFEWVBWCEEM2SoiicffbZqKoa6fUuxIkU\ncxW7N1gJgHrIMq3VdCNEqWc/aXGtZQEYIUSzkZ+fz1//+lcMwwDgtttu46233iIrK6ueVwpx7GIu\nsWt6AIAkV0atfeXeAgwzRHpi26YOSwghjuivf/0rzzzzDEuWLAGIzPsuRGOIuab49IS2lHsLSHTW\nTuzF7j0AZCTkNHVYQghRg8fjIT4+PDpn1qxZDB8+nLFjx0Y5KnEyiLmK3a+5AYhzJNbaV1K1F4B0\nSexCiCh6//336d+/P+vWrQOgVatWXHXVVVKliyYRc4m90leMVbXVOdSt2J2PRbWSGie9S4UQ0ZOS\nkkIoFGLPnj3RDkWchGIqsRumQZW/FGsdST2ka5R5C0iLb4OqSsc5IUTTMU2Td955h8rKcOfe8847\nj3Xr1nHhhRdGOTJxMoqtxG6Eh4ZkJNTuHFfq2Y9pGnJ/XQjR5BYuXMjNN9/MI488EtmWnJwcxYjE\nySymOs+ZhIeKKErtzyMlBzvOpdeR9IUQ4kQzD06moSgKl1xyCbm5udx+++1RjkqIGKvY3YEyAAKa\nr9a+SI/4I8xIJ4QQJ0pBQQHXXHMN77zzDgB2u52//e1vsryqaBZiKrFX+koBCOq1E3uJew9Wi73O\n8e1CCHEi+f1+vvrqKz7++ONohyJELTHVFB84ONTNqtprbNdCAcq9RWQndUSto5leCCGOV0FBAX6/\nnw4dOtChQwc+/fRTunfvHu2whKglprKgboQA6JI9sMb2Es9ewCQjUTrOCSFOvP379zN48GBuvPHG\nyPzuPXv2RFVj6k+oOEnEVMVePZ1s+mHrsBdXyYxzQojG07p1ay666CL69Okjk8yIZi+mEnv1AjB2\nS82V20rcMuOcEOLEev/999m+fTt33303AM8++2yUIxKiYeptR6qoqGDOnDmRX+7PPvuM0tLSRg+s\nLgUVOwGwWZ01the792C3ukh0pkUjLCFECxMIBHj00Ud57rnnKC4ujnY4QhyTehP7jBkzaN26dWRq\nxGAwyJ/+9KdGD6wuqmolyZVJnP2XeeIDIS9V/hIyEnKkiUwIcVwKCgoAcDgcvPrqq6xYsYKMDBlp\nI2JLvYm9tLSUyZMnY7PZABgzZgx+v7/RA6uLpgewW2pW69ULv8j9dSHEr2WaJrfddhvnnnsuZWXh\n+TJOPfVUOnfuHOXIhDh2DerSqWlapBouLi7G6/U2alBHYpghTMwa24plxjkhxHFSFIXu3bvToUMH\n3G53tMMR4rjUm9ivueYaLrvsMrZt28Ytt9zChAkTmDp1alPEVqcKb2GN57/MOCcVuxCi4UpLS3nm\nmWciU8P+/ve/5+OPP5bZ40TMq7dX/AUXXMCAAQNYs2YNdrudv/71ryQlJTVFbHWyqDVDLnHvwWlL\nIM4uCy4IIRruz3/+MwsXLqRjx45cfPHFWCyyKqRoGepN7FOnTuW1115j7NixkW2XXnop7777bqMG\ndiQJztTIY7/mwROoICe1u3ScE0LUS9O0SH+hhx56iD59+jBu3LgoRyXEiXXExL548WJeeOEF9u3b\nx7nnnhvZrmlaVHuJxjtSIo99wSqgZrIXQoi6LF++nDvvvJO33nqLvn370rp1a6ZNmxbtsIQ44Y6Y\n2MePH8+FF17In//85xpLEaqqSnZ2dpMEV5dDe8X7NQ8ADmt8tMIRQsQIwzAoKipi06ZN9O3bN9rh\nCNFojtp5zmKxMHv2bFJSUlAUBUVRCAQCXHHFFQ06+WOPPcaVV17JxIkTWb9+fY19+/fv56qrruKy\nyy7joYceanDA1bPPAQRC4cTutMU1+PVCiJPH8uXLqaoKt+yNGjWKNWvWMHHixChHJUTjqrdX/Kuv\nvsqwYcMYM2YMl1xyCb/97W/p1atXvSdetWoVu3btYuHChTz66KM8+uijNfbPnj2bG264gUWLFmGx\nWNi3b1+DAm6V3Cny2K+Fh91JxS6EONzixYu5/PLLmTVrVmRbq1atohiREE2j3sS+ZMkSvv76a/r3\n78+3337Lk08+SdeuXes98TfffMPIkSMB6Ny5MxUVFZHxoYZhkJuby/DhwwGYOXMmbdq0OeK5DuU4\npDoPVDfFS8UuhDjM6NGjufzyy7n++uujHYoQTareXvHx8fHY7XY0TQNgxIgRXH/99Vx77bVHfV1x\ncTG9e/eOPE9LS6OoqIiEhARKS0uJj4/n8ccfZ9OmTZx++uncddddDQp43+5C3Htzw4+DeQDs3Lab\nA2pVg14vji43NzfaIZwU5H0+8TweD3//+98ZMGAA5513Hhs3buSmm27C5/PJ+91I5H1tnupN7MnJ\nySxevJhu3bpx//3307lzZwoLC+t7WS3Vk0BUPy4oKGDy5Mm0bduWm266ic8//7xG7/sjyWqbSu+2\n4fXYPVu2UVIEp/YdKD3jT4Dc3FwGDhxY/4HiuMj73Dh27tzJl19+ic/n49xzz+X000+Pdkgtmvwe\nN75AIMDGjRuP+XX1NsXPmTOHAQMGcP/999OhQwcOHDjAU089Ve+Js7KyaqyKVFhYSGZmJgCpqam0\nadOG9u3bY7FYOOuss/j5558bFHCSKz3yuLrznMMm99iFOBlVVlayd294vYhTTjmFDz/8kH/9618y\nr4U4qdWb2P1+Pzk5ObhcLm655RZmzJhBQkJCvSceMmQIS5cuBWDTpk1kZWVFXme1WmnXrh15eXmR\n/aecckqDAnbZfrm2X/NiUa1YVVuDXiuEaDmKi4sZMmQIv/vd79B1HYCBAwdGJqAR4mR1xKb41atX\nc+eddxIIBEhLS+OVV16hffv2/POf/+SVV17hyy+/POqJBwwYQO/evZk4cSKKojBz5kzee+89EhMT\nGTVqFA888AD33XcfpmnSrVu3SEe6+lgOSeIBzYvDGiefzoU4CWVkZHDmmWfSqVMnDMOQKWGFOOiI\nif3pp5/mjTfeoHPnzixfvpwHH3wQwzBITk7mnXfeadDJ77777hrPe/ToEXncoUMH3n777V8Zdlgg\n5CHBmXZc5xBCxI4vv/ySDRs2RGaMe+WVV+SDvRCHOWJTvKqqkbWIR4wYwd69e5k8eTLz5s2L6sxz\nqhL+VK4bITQ9gFPGsAtxUtA0jTvvvJNZs2Zx4MABAEnqQtThiBX74f9hWrduzahRoxo9oKNx2RJx\n2sOJPBA6ODmNjGEXokWrrKwkKSkJm83G3//+d6xWq0w0I8RR1Nt5rlpz+GQc70jBYQ0n8sDBWedk\nOlkhWq4ZM2Zw1llnUV5eDsCgQYM47bTTohyVEM3bESv2NWvW1BhXXlJSwrnnnotpmiiKwueff94E\n4dVU3QwPsgCMECeD1NRUEhMTKSoqIiUlpf4XCCGOnNiXLFnSlHE0SJnnAMGQH7vVKU3xQrRAHo+H\nhQsXMmXKFBRF4Y477mDatGk4nc76XyyEAI6S2Nu2bduUcTSIZvgjj6srduk8J0TL8cADDzB//nyS\nk5O59NJLsVqtWK31TpAphDhEzP2PqW6OlwVghGgZDMNAVcPdfe655x4yMjK44IILohyVELGrwZ3n\nmovqPwDSFC9E7Pv+++8ZOnQoP/74IwA5OTk8+OCDuFyuKEcmROxqUGL//PPP+ec//wnA7t27ayzo\n0tSUgyFXr8UuTfFCxK7S0lI2b97MypUrox2KEC1GvU3xf/vb39i1axf79u1j0qRJ/N///R+lpaU8\n+OCDTRFfDQpqZNidLAAjRGxavXo1PXv2JD4+ntGjR7Nq1arIZFhCiONXb8X+/fffM2/ePOLjwwl0\n2rRpbNq0qdEDq0v1GHYIj2NXFVkARohYsmzZMsaMGcOsWbMi2ySpC3Fi1ZvYHQ4H8MsENbquR1ZS\namoZiTmRx37Ni9MmC8AIEUuGDh3KqFGjmDBhQrRDEaLFqrcpfsCAAdx3330UFhby+uuv8+mnn3LG\nGWc0RWy1HJrEZQEYIZo/v9/PnDlzOPXUU5kwYQJOp/O4F38SQhxdvYn9zjvvZMmSJbhcLg4cOMCU\nKVM4//zzmyK2Wty+MuDQBWCkR7wQzdm+fft4+eWX6d27N+PHj5cWNiGaQL2Jffr06UyYMIEHH3ww\nMtQsWqr8JcChQ92k45wQzU0gEKCiooKsrCw6derEggULGDBggCR1IZpIvZn63HPP5e2332b48OE8\n8sgjbNiwoSniOipZAEaI5qm8vJzhw4dzww03YBgGAOeccw4JCQlRjkyIk0e9Ffv48eMZP348VVVV\nLFu2jJdeeondu3fz0UcfNUV8NTjt4T8OvywAI4ldiOYkOTmZzp07k5mZSTAYlDnehYiCBk0pa5om\nP/74Ixs2bGDnzp307t27seOqU2Q6WWmKF6LZWL9+Pbm5uZGFW/7xj3/I/O5CRFG9//seeughPv/8\nc3r16sWFF17IvffeG7XpHhWletY5WQBGiOZA13WmTp3Krl27GDVqFDk5OZLUhYiyev8Hdu/enTvv\nvJPU1NSmiOeofEE3IPPECxFtfr8fp9OJxWLh2Wefxe/3k5OTU/8LhRCN7oiJ/eWXX+bmm29m7dq1\nrFu3rtb+J554olEDq0ublPAMVbKymxDR8/TTTzN//nw+//xzkpKSGDx4cLRDEkIc4oiJvVevXgB1\n/qeN1rCVX5riZQEYIaIlGAwSDAbZtWsXffv2jXY4QojDHHG429ChQwHYvn07v/3tb2v8+/7775ss\nwENpoQAgneeEaEqapvHuu+9GVnW88847WblypSR1IZqpI1bsy5Yt49NPP+Wbb76hsLAwsl3TNFav\nXt0kwR2uxLMXCDfFywIwQjSNBx98kFdeeQXDMLj88sux2+3Y7fZohyWEOIIjJvahQ4eSlpbGxo0b\nOeussyLbFUXh9ttvb5LgDld9C0AWgBGi6UybNo1AIMCoUaOiHYoQogGOmNidTicDBw7kgw8+QNM0\nEhISKC4uJi8vj44dOxgTnq0AACAASURBVDZhiL+IsycDsgCMEI1p8+bN3HXXXcydO5cePXrQrl07\nnn766WiHJYRooHqnlH3iiSf45P+zd+fhMZ3tA8e/M0kmiWwSxBJ71F4tKrVvjZfaiiJJSdROEfV6\nkdjVVmopobqg1FKU2IJ4S6Ootai1qFgTKkEiezJJ5vdHfjmvkZUmmUxyf67L1czMmXPueZrMfZ7n\nPOe5DxwgMjISd3d3Nm7cyMyZMwsgtIwszKykAIwQ+ez27ducPHmSffv2GToUIcRryDGxX7t2jT59\n+nDgwAF69uzJl19+yb179woitgzUKhOZOCdEPrh58yZxcWl/W507d+bw4cOMHz/ewFEJIV5Hjok9\nfSbskSNHaN++PZB2u4shRCc8lQIwQuSx48eP06ZNG+bOnas817BhQwNGJIT4J3JM7NWqVaNz587E\nxsZSp04ddu3ahZ2dXUHEloG1ub0UgBEijzVu3JhGjRrpTZIVQhivHJeUnTNnDjdv3sTZOW3Vtxo1\nahhk1TkAjamlDMUL8Q+lpKTw9ddfKyftlpaWBAQEyF0mQhQROSb2hIQEfvnlF5YtW4ZKpeLtt9+m\nRo0aBRFbBipU/xuKl1XnhHgt9+/fZ86cOTg7O9OpUyfUarUkdSGKkByH4qdNm0ZMTAzu7u707duX\nJ0+eMHXq1IKILYNnsQ9JSJZ14oV4VampqURGRgJpl9fWrVvHrl27UKtz/AoQQhiZHHvsT548YcmS\nJcrjdu3a4enpma9BZUWHFIAR4lVFRUXh7u6ORqPB398ftVpNx44dDR2WECKf5Hi6Hh8fT3x8vPI4\nLi6OxMTEfA0qK5ZmVjIUL8QrsrGxwc7ODjs7O72/ZSFE0ZRjj93NzY3333+f+vXrA3D16lXGjh2b\n74Flxt6qPOFxdwHpsQuRnbt373L69Gnc3NxQqVSsXbsWCwsLuZYuRDGQY2Lv3bs3LVq04OrVq6hU\nKqZNm0bZsmULIrYM1Gr1CwVgpAiFEJlJTU3F3d2d27dv4+LiQrVq1bC0tDR0WEKIApJtYv/111+5\nffs2jRs3xtXVtaBiylJMQiSJyVIARojMpKSkYGJiglqtZsGCBTx58sRgdR2EEIaT5TV2Pz8/Vq1a\nRVhYGFOnTmXPnj0FGVemtClJJGhjZXEaIV6ybt06WrVqRVRUFABt2rThww8/lBNgIYqhLHvsx48f\nZ9OmTZiamhIdHc2YMWPo3r17QcaWgaWZdVoBGFmcRgg9oaGhPHr0iOvXr+Pi4mLocIQQBpRlj12j\n0WBqmpb3bWxsSElJKbCgspKiSwZk1TkhdDodP//8s1LLYcKECZw8eVKSuhAi68T+8hBeYRjSi0tM\nW2BDhuJFcTdnzhzc3NzYvn07kHYiXq5cOQNHJYQoDLIcig8ODmbixIlZPjbEevHa5LT756Wymyju\nPD09CQ4OpkWLFoYORQhRyGSZ2P/zn//oPS4MlZ90//9fGYoXxU1ISAg+Pj7MmDGDN954g6pVq7Ju\n3TpDhyWEKISyTOw9e/YsyDhypaSVI49jb8lQvCh2Lly4wP79+6lRowYzZ840dDhCiEIsxwVqCpOU\nlLTJczIrXhQHDx8+xMHBAQsLC7p168bOnTtp3bq1ocMSQhRyRlXaKV4KwIhi4uzZszRv3pz58+cr\nz7Vp06ZQTGIVQhRuuUrsERERXL58GUhbrtJQ4hIjAJkVL4q+unXrUrVqVZydnQ0dihDCyOQ4FB8Q\nEMDy5cvRaDQEBAQwe/Zs6tatS58+fQoiPn3/31uRoXhR1Oh0OrZt24aDgwMdOnTAysqKX375Reql\nCyFeWY7fGt9//z27d+/G3t4egEmTJrFt27Z8DywzySlaKQAjiqQHDx4wduxYJk+erCwGJUldCPE6\ncuyx29jY6FWGsrCwwMzMLF+DykpKSpIUgBFFhk6nIy4uDisrKypXrsxXX33FO++8g4mJiaFDE0IY\nsRwTu729PTt37iQxMZGrV6+yf/9+HBwccrXzefPmcfHiRVQqFZMnT6ZBgwYZtlm8eDF//PEHGzZs\nyHF/ybpkrEytc3VsIQqzuLg4hg8fTmxsLDt27EClUtGrVy9DhyWEKAJyHOubNWsWly9fJjY2lqlT\np5KYmMicOXNy3PGZM2e4d+8eW7duZe7cucydOzfDNrdu3eLs2bO5DlZHqlxfF0WCpaUliYmJaLVa\noqOjDR2OEKIIybHHbmtry/Tp0195xydPnlRquDs7O/P8+XNiYmKwtv5fj/vzzz9n3LhxrFixItf7\nlVvdhLEKDw/n1KlTVKhQAZVKxerVq7G2tpZr6UKIPJVjYs/q3tkjR45k+74nT55Qr1495bGDgwPh\n4eFKYvf398fFxQUnJ6dXCjgqIo5z58690ntE7knb5g+dTsfIkSO5d+8e3377raHDKRbkdzn/SRsX\nTjkm9s2bNys/a7VaTp48SWJi4isfKL28JEBkZCT+/v58//33PH78+JX2U6lCFRpWafzKxxc5O3fu\nHI0bS9vmJZ1Op5wYz58/n1u3buHk5CTtnM/kdzn/SRvnv8TERK5cufLK78txDNDJyUn5V7VqVTw8\nPDh27FiOO3Z0dOTJkyfK47CwMMqUKQPAqVOnePbsGf369WP06NFcvXqVefPm5SpgWZxGGIs9e/bw\nr3/9i5iYGABcXV0ZMWKEDL0LIfJVjj32kydP6j3++++/uX//fo47btGiBX5+fri7u3P16lUcHR2V\nYfhOnTrRqVMnIK1qla+vL5MnT85VwFLZTRiLixcvcvXqVc6dO0ebNm0MHY4QopjIMbF/9dVXys8q\nlQpra2tmzZqV444bNWpEvXr1cHd3R6VSMWPGDPz9/bGxsaFDhw6vHbDMiheF2dmzZ3nnnXdQqVRM\nnDiRjz76SJaFFUIUqBwTu4+Pj94kuFfxck332rVrZ9imYsWKubqHPZ0MxYvCaunSpcyePZvVq1fT\nq1cvzM3NJakLIQpcjhf7FixYUBBx5JrG1MLQIQiRqQ8++IDWrVu/9omwEELkhRx77BUqVMDT05O3\n3npLbynZsWPH5mtgWTFRG2Y5WyFeFhERwfTp0xk3bhzVq1enevXq7Nq1y9BhCSGKuRwTe8WKFalY\nsWJBxJIrppLYRSFx9OhRNm3ahIWFBV988YWhwxFCCCCbxL5nzx66d+/O6NGjCzKeHJmYSGIXhhMZ\nGYmFhQUWFhZ0796ddevW0blzZ0OHJYQQiiyvsW/fvr0g48glFSaqHAcZhMgXV65coUWLFixcuBBI\nu0uke/fumJrK76QQovAwqpUy1CoTKdkqDKZq1apYW1tja2tr6FCEECJLWXY1Lly4QNu2bTM8n75E\nZk5rxecHNZLURcE6dOgQpqamtG3bFmtra44dO4ZGozF0WEIIkaUsE3vdunVZsmRJQcaSI5XKqAYY\nhJF7+PAh/fr1o0KFCpw9exZTU1NJ6kKIQi/LxK7RaF658lp+U6vlWqbIf1qtFjMzMypUqMCSJUt4\n66235Dq6EMJoZPlt1aBBg4KMI1fU0mMX+SgxMZFJkybx6NEjtmzZgkqlol+/foYOSwghXkmWmXLC\nhAkFGUeuqFUmhg5BFGEajYYHDx4QGhpKRESEocMRQojXYlTji7I4jchr0dHRnDlzhvfeew+VSsW3\n336LjY2NXEsXQhgtoxrbtjIvaegQRBGi0+no3bs3/fr148aNGwCUKlVKkroQwqgZV49dVp0TeUil\nUjFu3DjOnTtH1apVDR2OEELkCaPqsSenJBs6BGHkjh8/zocffkhsbCwAnTp1YsqUKZibmxs4MiGE\nyBtGldiTUuINHYIwcj///DO//vorx48fN3QoQgiRL4wqsct97OJ1XL9+XfnZ19eXQ4cO0bFjRwNG\nJIQQ+ceoEruJcYUrCoHVq1fTokULdu/eDYCFhQVvv/22gaMSQoj8Y1SZUiX3sYtX1KZNG+rXr0+F\nChUMHYoQQhQIo0rsJmpJ7CJ7cXFxzJgxg7t37wLwxhtvcOTIEZo0aWLYwIQQooAYVWJXS2IXOTh0\n6BB+fn588cUXynNS6lcIUZwY1Ww0R9sqhg5BFELx8fGYmJig0Wjo1q0by5Yt48MPPzR0WEIIYRBG\n1WPXqOVeY6Hv1q1btG3bVumhq1QqPD09KVGihIEjE0IIwzCqxJ5q6ABEoVO2bFmSkpJISEgwdChC\nCFEoGNVQfETsQ6C+ocMQBnbu3DkSExNp3rw5NjY2HDt2DGtra0OHJYQQhYJRJXa1yqjCFfkgLCyM\nrl27UqZMGc6ePYu5ubkkdSGEeIFRZUq12qiuHIg8lJqailqtxtHRkdmzZ1OrVi1Z310IITJhXIld\neuzFTnJyMgsWLODGjRusX78elUrFkCFDDB2WEEIUWkbVBTZVS9nW4katVnP27FkuXrxIWFiYocMR\nQohCz6i6wCZSBKZYSEpK4vz58zRt2hS1Ws3XX3+NlZUVNjY2hg5NCCEKPaPqsZexqWToEEQB8PDw\noGfPnty4cQOAcuXKSVIXQohcMqoucAlzO0OHIArAoEGDqFSpEuXLlzd0KEIIYXSMqscuiqYrV67w\n8ccfEx8fD0CXLl348ssvsbW1NXBkQghhfIwqsYdG3DR0CCIfbNmyhT179hAYGGjoUIQQwugZ1VB8\nSmqyoUMQeSQkJISKFSsCMHnyZFxdXWnbtq1hgxJCiCLAqHrsSPXNImHr1q00btyYffv2AVCiRAlJ\n6kIIkUeMK7FLZi8S3n77bZycnLC0tDR0KEIIUeQYVWKXtG6ckpOTWb58OQ8ePACgVq1anDlzhvbt\n2xs4MiGEKHqMKrFLajdOBw8eZObMmcyaNUt5ztTUqKZ3CCGE0TCqb1dri5KGDkHkUkpKCqmpqZiZ\nmdG5c2fmzJmDh4eHocMSQogiz6h67PYlyhk6BJELISEhdOrUiSVLlgCgUqn45JNPsLe3N3BkQghR\n9BlVYhfGwdbWlkePHnH//n10Op2hwxFCiGLFqIbiI+L+ppx5FUOHITJx69YtIiIiaNKkCba2thw5\ncoTSpUsbOiwhhCh2jCqxxyVGGToEkYmIiAjat2+Pvb09p06dwtLSUpK6EEIYiFEldlE42dvbM2HC\nBCpXriz3pgshhIFJYhevLDU1lW+//ZYLFy7w9ddfo1KpGDNmjKHDEgaQnJxMamqqocPIVFJSkqFD\nKPKkjfOGWq3O01uAjWrynNzFXnjs27ePw4cPExoaauhQhIFER0cX2i92Z2dnQ4dQ5Ekb552kpCSi\no6PzbH9G1mOX1G4oqampXL16lTfffBO1Ws2qVavQaDQ4OjoaOjRhAMnJyZiYmFCiRAlDh5IprVaL\nRqMxdBhFmrRx3tFoNMTFxZGcnJwnPXej6rGbmJgZOoRia/DgwXTs2JFbt24BULFiRUnqxVhqaqqs\nHihEHjIxMcmzy1pG9ZdZ1raqoUMotnr06IFWq8XW1tbQoQghRJGjUuXdiHS+9tjnzZuHm5sb7u7u\nXLp0Se+1U6dO0bdvX9zd3fH19S20E3CKq/v37zN27FgSEhIA+OCDD9iwYYP00oUQopDLt8R+5swZ\n7t27x9atW5k7dy5z587Ve3369OksX76cLVu2EBsby7Fjx3LcZ7w2Nr/CFS/59ttv2bBhAzt27FCe\ny8szSiH+qZCQEBo2bIinpyeenp64ubkxbdo0UlJSAIiPj2f69On06NGD3r17M2LECB49eqS8/+7d\nuwwbNozevXvTq1cvZs+enevJgHv37qVjx478/vvv2W53+vRpvL29X/9DZsLPz4+NGzfm6T4z06lT\nJ73v7ZCQEHr16qW3jb+/PwsWLADS5l0sWrSIHj164OHhgZeXFzdu3HitY2fXKQQ4dOgQH374IR4e\nHkpb/PTTT8rvgqenJw0bNnytYxcF+TYUf/LkSVxdXYG02ZPPnz8nJiYGa2trIO0XIv1nBwcHIiIi\nctzn89jHlLR2yK+Qi72oqP8tAOTr60vjxo3p0aOHASMSInvVqlVjw4YNymMfHx/27t1Lhw4dmD9/\nPo6OjuzatQuAc+fOMWTIEHbt2oVarWbMmDFMmzYNFxcXdDodc+bMYeXKlYwbNy7H4544cYIJEybw\nzjvv5NtnM6QrV66g0+k4ePAgvr6+qNU59wFXr15NVFQUO3fuRKVScf78eUaPHs2BAwdeaT7Gi53C\n4OBgJk+ezNatW5XXU1NTmT17Njt37qRkyZIMHToUV1dX+vTpQ58+fZR9HDhw4NU/eBGRb4n9yZMn\n1KtXT3ns4OBAeHi4kszT/xsWFsZvv/3G2LFjc96pdBjzzb59+xg+fDhr1qyhY8eOWFlZ0bNnT0OH\nJYzExL3n2H7xXp7us/dbVVjYrfErvadBgwbcu3dPGQX8+eefldcaN25MgwYNOHz4MCVKlKB69eq4\nuLgAaaNREyZMyJDAtFot06dP58GDByQlJeHt7Y1KpeLo0aNcuXIFW1tbZR8Ac+bM4dKlS5iYmOiV\nKQZYu3YtBw8eJDU1lTZt2jB69GiuXbvGrFmz0Gg0aDQali5dSkhISIbncjO3Zf369ezfvx+A9957\nj44dOzJ79mxWr17N+fPnGTZsGGfOnCE1NZUePXoQEBCQ5b4CAgLo06cPhw4d4syZMzRt2jTH42/Z\nsoU9e/YoI3uNGjVix44dekn98ePH/Oc//9F735tvvsnEiROVxzl1CiMiIrC1tcXBIa2T17RpU06c\nOKE3mrBy5UoWLVqUY8xFVYFNnsusGMjTp08ZMWIEM2bMyFXlr7t37/HkQVx+hFfsxcfHo9FouHr1\nqiwHWwDOnTtn6BD+MWdnZ7RaLZCWAPO64I9WqyU2NuvLb/Hx8aSmpirbaLVa/vvf/9K7d29CQkKo\nXLkyiYmJJCYm6sV848YNLC0tcXZ2zrD/lJQU5TNBWoJTq9V88803hIeHM3ToUHbt2kWzZs147733\nqFevnrKP06dPExISwvfff8+5c+fYvXs3Li4uJCcnExsbS1JSEt999x1qtZpu3brRp08ftm7dSq9e\nvejatStnzpzh/v37/PTTTxmeq1atmhJTUlISiYmJerGHhoayY8cOZfTCy8uL1q1b8+jRI2JiYjh1\n6hS1atXi0qVLaLVa6tSpk2Xbpqamsn//ftauXYtKpWL37t28+eabGdobIDExEa1Wy99//42ZmRkm\nJiZ6r7/82Nramq+//jrDMV/c5tGjR9SoUUN5zs7Ojvv371OlSlqdEHNzc6Kjo/nzzz8pX748J06c\n4J133lG2v3r1KmXKlKFEiRLZ/v4UNlqtluDg4DzZV74ldkdHR548eaI8DgsLo0yZMsrjmJgYhg4d\nyqeffkrLli1ztc+qVatSxbF2nsdaHOl0OjZt2kTbtm2pWLEijRs3pkKFCjRv3tzQoRV5586do3Hj\nV+uJFjbp16LT72Ne2qspS3tl9468Z2lpyb179xgxYgQAN27cYMiQIXTt2pULFy6gUqmwsrLSe4+Z\nmRmWlpaYm5uj1WozvP6yv/76ixYtWmBlZYWVlRUWFhZotVpMTU2xsLDQe39wcDAuLi5YWVnRunVr\nWrduzenTpzE1NcXKygpbW1uGDx+OqakpkZGRaLVaOnXqxMyZM3n06BGdO3emZs2aREdHZ3juRRqN\nBnNzc71j3717l4YNG2JnZwfAO++8w/3796lduzZhYWFcv36d/v37c+PGDRISEpTPlJlTp07h5ORE\njRo1sLe3p3v37mg0GiwtLVGr1cr7YmNjMTc3x8zMDCsrK3Q6XY7tmRtmZmZ6n0+tVmNpaam374UL\nFzJ79mxsbGyoUqWKEgP8b7QhL2IpSElJSbz55pt6awMkJiZy5cqVV95Xvk2ea9GiBQcPHgTSzqAc\nHR2VoRSAzz//nAEDBtC6dev8CkFk4+eff8bb25spU6Yoz5mbmxswIiFeXfo19g0bNtC0aVOlZ+vk\n5MSdO3cyTIa7fv06zs7OVK9encuXL+u9lpSUxM2bNzMc48WRiKSkpCyvN2d3H3JoaCjr1q1j9erV\nbNiwAScnJwCaNWvG9u3bqV69Oj4+Ppw6dSrT53KiUqn04tRqtajValxcXLh48SIJCQm8++67/PHH\nH5w/f5533303y30FBAQQGhrKBx98wKBBg4iPj+fEiRPY29sTExOjt+2zZ89wdHTExsaG5ORkvc4c\npH33vxjX48eP9Sa4eXp6snDhQr335NQpBHBxcWHz5s1888032NjYKO0JaSMnxXniHORjYm/UqBH1\n6tXD3d2dOXPmMGPGDPz9/fn555+Jj49n165dbN++Xfmf++LkiKyo5CL7P6LT6ZQZwx06dGDixIkZ\n7lYQwlhNmDCBRYsWER8fj5WVFe3atWPFihXK6+fPn+fatWu0bduWFi1aEBoayi+//AKkDT9/8cUX\nyjXqdG+++SanT58G0oaI1Wp1lte7X9w2/dp5uoiICBwcHLCysuLq1auEhoai1WrZuHEjkZGRdO/e\nnQEDBvDnn39m+lxO6tSpwx9//EFycjLJyclcvHiROnXq0KRJE3bv3k3lypWVScrPnj2jfPnyme4n\nKSmJoKAgdu/erfybPn06AQEBWFlZ4eDgoNwJEB8fT2BgoDLK169fP+bPn09ycjKQNjLl4+Ojd3JV\ntmxZ5UQs/d+L19ch504hwJAhQ3j69ClxcXEEBQXRrFkzIO3EwcrKqtiviJev19hfniRRu/b/htFf\nZ3ihlLVTzhuJTIWFhTFmzBhcXFwYP348KpUKHx8fQ4clRJ6pVKkSHTt2ZNWqVQwfPpzJkyezePFi\nZSjZwcGBZcuWYWJiAsCaNWuYPn06K1asQKPR0Lx5c0aPHq23zy5dunDmzBk8PT3RarV89tlnWR6/\nSZMmHD58mI8++giAGTNmEBkZCaQlXisrK9zd3WncuDHu7u7MmjWLQYMGMXbsWGxsbNBoNMyfP59r\n165leO5lP/zwg5L87OzsWLFiBW5ubvTv3x+dTkefPn2UXuytW7eU2eK2trbKHJrw8HD8/Pz0PtPR\no0dp3Lix3pynjh07smTJEhITE5Uh8GXLlpGYmMjAgQOpVasWkJZsv/76a3r27ImdnR02NjasWrXq\nlUcCX+wUqlQqZsyYAaTdSWVjY0OHDh3o27cvgwYNQqVSMWzYMGUiXXh4uPJzcabS5fWMl3yQfp2h\nfv36Mlz8miIjI2nRogVvvvkmP/74Y6b3pBeFa7/GoCi088vX2Aub2NhYo7vGaggLFixg0qRJr/Ve\naeO8ldnf1OvmPqNaUjZVl2LoEIzKo0ePCA8Pp0GDBpQsWZKDBw/i5OQkC80IIUhKSqJFixaGDkPk\nA6MqAvMkOsTQIRiNqKgoWrduzYABA5RbPipWrChJXQgBpPUMc3tHkjAuRtVjF7lna2vLsGHDKFWq\nFJaWloYORwghRAGRxF5E6HQ6duzYwW+//cbSpUuBtFnCQgghihejGoqXYeSs6XQ61qxZw08//cTd\nu3cNHY4QQggDMarELvTpdDru3LkDpK3OtGrVKo4dO0bVqlUNG5gQBUCqu/1P+/btiY2NVdYKyYqP\njw9BQUGvdLzp06fzwQcfZDheXNz/lvd+ufLbrl276NWrF+7u7vTu3ZvAwMBXOma6PXv28OGHH9Kn\nTx9++umnDK8HBwfTr18/+vfvz9SpU0lOTubKlSt6C+A0a9aM8+fPv9bxjZVRDcXLAjX6/v3vf/PT\nTz9x7NgxqlWrJgldFDtS3U3fy2VV/ymtVssvv/yCRqMhODgYZ2fnHN9z7tw5Nm3axLp167C1teXp\n06e4u7tTs2ZNqlevnutjx8XFsXLlSrZv346ZmRm9e/emQ4cOlCxZUtlm0aJFDBs2jDZt2rBy5UoO\nHDhAt27dlN+JqKgoPvnkE95+++1X//BGzKgSu5V5zoViipOWLVty48aNXJVUFKI4KM7V3SCtN29v\nb4+bmxsTJkzg4cOHNGzYkAMHDnD06FEgbRRh48aNPHr0iEWLFlG3bt0s93fs2DHq1q1LnTp12Ldv\nX65GHzZu3Mjo0aOVmEuVKsWOHTsyfAZvb2+9ct1mZmasXbtWeXzx4kXefPNNbGxsgLSFa86fP0/7\n9u2Vbe7du0eDBg0AaNWqFZs3b6Zbt27K62vWrGHAgAHF7jvSqBK7pcY6542KsCdPnrB8+XKmTJmC\nubk5vXr1omfPnsXul1YUPmfv7Ofuk0t5us+qpRvQpFrnXG+v1Wo5fPgwHh4ehISEUL169Qx1wOvU\nqcOdO3ewtLSkTp06eq9ZWFhk2Oe+ffvQaDRs3LiRx48f4+XlxcGDB2nVqhUdO3bUS+onTpzg77//\nZtu2bZw9e5b9+/crS52m27x5M2q1mvfee4+PP/4Yf39/PDw86NGjBydPniQ8PDzT515Oii+uPAdp\nK6696NixYyQmJrJt2zaCgoJYv3698ppKpWLNmjVs2bKFnTt3ZpvYAwIC6Ny5M3Xr1mXMmDG5Suy3\nb9/WW2UUyPTEZPny5dnu58mTJ3qryKWX/n5RzZo1+fXXX+nRowfHjh3TW2M+ISGB48eP564keBFj\nVIm9uFu+fDkrVqygSpUqDB48GJVKJRMKRbF2584dPD09gf9Vd3N1deXChQvKtfYX6XQ6TExMUKlU\nmb7+sitXrigFU8qWLYtGo1GWiX3Z1atXadSoEZC2vGyTJk2UteMh7cShf//+mJqaEhERQWRkJO+9\n9x4zZ87k7t27dO7cGWdn50yfe5mXlxf9+/dXHr/Yi4W0a8/psbRp00bvBCd91cOyZcty8eLFLD97\nXFwcv/32G5999hnW1tZKWed69eplun36d5FKpcqyGM4/kdkiqZMmTWLmzJn4+/srl1TSHTp0iLZt\n2xbLjo9RJfbnceE4mlc0dBgF6sVlGydOnEjVqlUZMGCAgaMSQl+Tap1fqXedV168xu7t7Z1pdbcX\nl+i8fv06rq6uaDQaNm3apLevpKQk7t69m6FMal5Wd9u5cydWVlZ07doV+F91t6CgIHx8fJg4cWKm\nzzVt2vSV2iX9+hqd6wAAIABJREFUBAYy3k2U/vzLn+1lhw4dIiUlhX79+gFphWz27dtHvXr1sLe3\nJzo6Wqm69uzZM+Xn6tWrc+nSJb1CM8HBwZQrV05vCdqchuIzq/L28rXy8uXL88033wBpoxRhYWHK\na0FBQXh4eGT5+YoyozqVSU7N3YzVouLXX3+lcePGHDp0CABra2sGDRqk94cphEhTnKu7vaxy5cpK\noa3jx4/nanTiZQEBASxcuFCp8rZlyxYCAwPR6XQ0a9aMgIAAIO3kYPv27UoJbi8vL1asWMHTp0+B\ntMsEn376qd4dCZA2AvlilbcXkzrAW2+9xeXLl4mKiiI2Npbz589nmKy4fPlyjhw5AqQViXlx5OLK\nlSsZLgkUF0bVY6eYzYovVaoUcXFxhIaGGjoUIQq94lTdLSft2rVjx44deHh44OLiojeTPDPjxo1j\n/vz5yjyDiIgIbty4oSRrSFuSulKlSpw/f55Ro0Yxc+ZM+vXrR0pKCi4uLri7uwPw9ttvM27cOAYP\nHoylpSWmpqZMmTKFGjVqvNJnsLCwYPz48cplx1GjRmFjY8Off/7Jzz//jLe3N127dmXixIn4+fnx\nzjvv0LZtW+X9UVFRGcq9FhdGVd2tQjUHyjtUM3Q4+SowMJAGDRpQoUIFAJ4/f46dnV2BHLsoVB0z\nBkWhnaW6W+EWGRnJ6dOn6dixI48fP2bAgAHZ3ku+ZMkSvL29M0w2zE5xb+O8VmyruxX1HvvRo0f5\n6KOP6NKli3LdsKCSuhCi6LCysuLAgQOsWbOG1NRUfH19s93+7bfffqWkLgo3o/o/WVTTuk6nQ6VS\n0apVK0aOHKk321UIIV6VmZkZX375Za63f3lWvTBuRjV5zswk432mxiwqKopRo0Yp93OqVCrmzp2b\n4f5aIYQQIreMKrFbWxStleeSk5M5fPgwBw4ceK1Zq0IIIcTLjGooviiIiori4cOH1K5dGwcHB/bs\n2UO1atXkFjYhhBB5wqgSe7w25pVmBhY2sbGxtGrVCjMzM44ePUqJEiUyLIYhhBBC/BNGldgTtDFA\nKUOH8dqsrKzo3bs35ubmmJmZGTocIYq08PBw/Pz8sr33vKjw9PQkLi6OEiVKKJNxZ8yYodw7vnfv\nXr7//nvMzMzQarUMHz6cjh07AmmXBL/88kuOHz+OpaUlZmZmTJkyhVq1ahnyI+l5/Pgxbdu2xc/P\nD1dXVyBtQZq//vqLSZMmKdv5+PjQsWNH2rVrx5MnT5gzZw73799HrVZTpUoVZsyYkeuCOumio6MZ\nP3480dHRlChRgsWLF2dYF2D58uUcO3YMExMT/vOf/ygL6cybN4/ff/8djUbDF198QaVKlf5hS+SO\nUSV2Y5wXHxQUxOHDh5kzZw4A06ZNM3BEQhQPZcqUKRZJPd38+fOVEcDTp08ze/Zs1q9fz4ULF1i3\nbh1r166lZMmSxMTEMHToUGxtbWnWrBmrV68mKiqKnTt3olKpOH/+PKNHj+bAgQOF5ha4ffv2UaVK\nFfbt26ck9pxMnDiRnj17KtXeVq9ezaxZs1i8ePErHXv9+vW4uLgwZMgQtm7dynfffceECROU169d\nu8aJEyfYunUr0dHRDB8+nC1btvDrr7/y4MED/P39CQoK4rffflMW8clvheP/Wi4ZW1rX6XR8/vnn\nXLhwAS8vLxl2FyIP+fv7c/bsWSIiIvjrr78YN24cAQEBBAcHM3v2bCpWrIi3tzf+/v789ttvLFmy\nBBMTEzp37szHH3/Mv/71L1q3bk2pUqXo2bMnkydPRqvVKnenvNy7OnHiBMuWLcPMzAxbW1u+/PJL\nxo0bx8cff0yTJk1ISEigc+fO/Pzzzyxfvpzff/+dlJQU+vfvT9euXfHx8cHMzIzIyEjmz5/P+PHj\niYuLIyEhgWnTptGgQQN27drFmjVrKFeuHPb29jRt2pQPPviAadOm8eDBA5KTk/H29s5QNe5lb731\nFvfu3QPSKsF5e3srvUxra2v+/e9/s3r1apo1a8aWLVvYs2ePsqZ8o0aN2LFjR4akvmvXLjZs2IBa\nrWbgwIG0adOGd999V1lG19vbm379+nHmzBkePHhASEgI9vb2uW6f7AQEBDB9+nTGjRunjExkJzg4\nmKioKL0SrgMHDiQhIUFvuyNHjrBmzRq95/r27av3vpMnTzJv3jwgbUW/ESNG6G1/9+5d6tWrh1qt\nxs7ODhsbG0JCQvjll1+U/bRr1y7bePOaUSV2Y0ntjx8/pmzZsqhUKlauXElsbKwkdVHk/XT280yf\nr+/UhjoV0hLR0RtbeRx1J8M2ZWwq07Z22lKsN/8+w8UHv9CniU+Ox7x79y6bN2/mp59+4ptvvmHX\nrl34+/sTGBjIkCFDgLQT7FmzZrFlyxbs7Oz45JNPcHd3Jzk5mdatW9O6dWt8fX3p3bs3nTt3JjAw\nkBUrVrBgwQK9Yz1//pxFixZRqVIlJk6cyPHjx+nQoQO//PILTZo04bfffqNFixZcuHCB0NBQNm3a\nRFJSEj179lR6mXZ2dsyePZs7d+7Qp08fXF1dOXnyJN999x3Lli1jyZIl+Pv7U6JECbp27UrTpk3Z\nu3cvZcqUYd68eTx79owBAwawd+/ebNslMDBQKcd6+/btDLfQppevjY6OxtzcPMPw9MuPY2Ji+Oqr\nr9izZw9JSUlMmjSJNm3aZHl8rVbL5s2b2bVrV67bJ7OyuenxR0dH07x5c959911++eWXHE8E7ty5\nk+Ezm5iYZFgpr23btnrL0GbmxfKxpUqV0is0A2mlY1etWkV8fDyxsbH8+eefPH36lNDQUK5evcrW\nrVuxsLBg+vTpODk5ZXusvGJUiV1lBIn9s88+Y/Xq1Rw7dowqVaq88vrIQojcq1+/PiqVijJlylCr\nVi1MTEwoXbo0MTExyjbPnj3D3Nxc+XJOrwYG0KBBAyCtYMj48eMBePfdd1m5cmWGYzk4ODB16lRS\nUlJ48OABTZs2xdXVlTVr1jBp0iQOHz5M586dOX/+PBcvXlTKyaampip1xNOPV7p0ab766ivWrFlD\nUlISJUqUICIiAmtra0qXLg2g9MovXLjAuXPnOH/+PJC2zOjLVesAfH19KVGiBGFhYVSsWFFZYz6z\nMqo6nU6pUpebW21v375N9erVsbCwwMLCglWrVhEbG5vl9umfs3379rlun6yuP6fXhAfo2rUr/v7+\n2Sb29JGH/LiFOLMV2GvUqIGbmxsDBw6kYsWK1K5dG51Oh06nw87OjvXr17N7924WLFiQYw36vGJU\nid0Yeuy1atWiUqVKxMXFGToUIQpUbnrYrWu55bhNzXIu1CznkqtjvjhcnNX1YLVanWU51fRJrCqV\nSvnS1mq1qNVqLly4wJIlSwBYtGgRkydP5ttvv8XZ2Vm5dm9ra4ujoyO3b9/mwoULfPbZZ9y6dYve\nvXszfPjwLI+3fv16ypYtyxdffMHly5dZuHChXrJNjyn9PSNGjMixl5p+jT0oKIht27bh6OgIpJVR\nvXLlCuXKlVO2/fPPP6lRowY2NjYkJyfz5MkT5YQC0mrL161bV4khuzZMp9VqM3zOV22fzOzbtw+V\nSsWRI0dITU3lwYMHREVF4eDgQFRUlN626eVjLS0tWbZsWYZ9pa+7ni43Q/GOjo6Eh4djY2PD48eP\nlXZ9Uf/+/ZUVQ93c3HBycqJ06dI0adIEgFatWumdUOY3o1qgxq5E6Zw3KmAxMTEsXrxYWcC/b9++\nBAUFyepxQhQS9vb2pKSk8PjxY3Q6HcOHD8+QEF4suXr27Fnq169Pw4YNlZKiZcuWJSYmhvLlyxMV\nFcXp06eVRNahQwe+/vprZb31Bg0aEBQURGpqKomJicyePTtDTBEREVSuXBlIq3uu1WopWbIkkZGR\nPH/+nISEBM6cOQOkXS8/fPgwAE+fPlVONrLSrl07kpKSlHKmXl5e+Pn58ezZMyDtO2vp0qV8/PHH\nAPTr14/58+eTnJwMpBUp8vHxUb7TIO3k4M6dO8TGxpKYmMjAgQOV2ffx8fHEx8dnWV72ddon3aVL\nl7CysiIwMJDdu3ezd+9e3n//fQ4ePEiDBg04d+6c8rnu3r1LSEgIb7zxBtWrV6dcuXJs2rRJ2df3\n33/P+vXr9fbftm1bvdKxGzZs0EvqAC1atFAK6Pz3v/+lVatWeq8/e/aMoUOHotPp+Ouvv0hNTaVM\nmTK0bt2aY8eOAWknFNWqFVwBMyPrsRc+S5cuZenSpZQoUYKRI0eiUqkKbcUrIYqrGTNm4O3tDcD7\n77+f4Rqyt7c3U6ZMYdu2bZiZmSmTpV700Ucf4eHhQdWqVRkyZAh+fn60a9cOV1dX5syZowzfN2rU\niHfffRc3Nzd0Op1SxvVFH3zwAZMmTSIwMJB+/foREBDA7t27GTlyJP369aNKlSrUr18ftVrN+++/\nz6lTp3B3dyclJSVDadnM+Pr6MmrUKJo1a6aUUR0yZIhyu5uXl5dyS9aQIUP4+uuv6dmzpzL5a9Wq\nVXprhpQoUQJvb28GDhwIwMcff4xKpcLDw4O+ffvi7OxMvXr1Mo0lt+1z9OhRQkJC9NorICCAXr16\n6e3vww8/ZOXKlfTp04dp06YxevRoTExMMDU15YsvvlC+f5cuXcpnn33Gtm3bKFGiBLVr11buTnoV\nnp6eTJgwgY8++ghbW1u++OILAObOnYuXlxeVKlWiTp06fPjhh6jVauUYnTp1YtasWbi7u2Nqaprt\nCUxeM6qyrXXq1KJECcPX19VqtcpQU1RUFN999x2jRo3KcvKHsSgK5USNQVFoZynbmj8CAwNp2rQp\nJUuWZPDgwYwaNYpGjRoZOqxM5XUbx8bG8v333+fqxKUoysuyrUY1FB+bFJXzRvns3LlzNGvWjKCg\nICDtGtL48eONPqkLIQwvISGBAQMG4O7uTuXKlQttUs8P4eHhyiQ58c/IUPwrMjU1JSQkhMuXLxf4\nvYlCiKKtR48e9OjRw9BhGETVqlUNHUKRYVSJ3VC3u506dYqqVatSrlw53nrrLS5cuED58uUNEosQ\nQgiRHaMaijdEXj916hRdunTRW0JQkroQQojCSnrsOXBxcaF///54eHgU+LGFEEKIV2VUib0gxMfH\nM3/+fMqVK8cnn3yCWq3OdKEDIYQQojAyqsRuaZb/t7rFxcWxdetWSpcuzbBhwwpNdSMhhMgNPz8/\n9u7dS9myZdHpdCQkJDB8+HA6dOgApC2WsnDhQuLj49Fqtbi6ujJy5EhMTEyAtGIvP/zwAxqNhuTk\nZIYMGUKnTp0M+ZEy6NSpE61atWLKlCkAhISEKAV/0r1Y1jUvS9POmzePixcvolKpmDx5srJ8brpD\nhw6xatUqNBoNXbp0UVak27NnD6tXr8bU1BRvb+8c16j/J4wqa5mY5E8N84SEBB49ekS1atUoVaoU\nP/30E87OzpLUhRBGycvLS0kokZGR9OjRg1atWpGcnMz48eNZtmyZsqb53Llz8fPz49NPP+XcuXNs\n2rSJdevWYWtry9OnT3F3d6dmzZpUr17dwJ8qzZUrV9DpdBw8eBBfX1+9ZXizklelac+cOcO9e/fY\nunUrwcHBTJ48ma1btyqvp6amMnv2bHbu3EnJkiUZOnQorq6umJubs3LlSnbs2EFcXBx+fn6S2PNT\nQkIC7733HsnJyRw5cgRLS8sMZ2BCiMInp7KtTZs2Zf78+Vy6dInExEQ8PDzo06cPoaGh+Pj4kJKS\nQoUKFViwYAFTpkxRSqouWbKE6dOn8+DBA5KSkvD29qZly5Z6x/7777+VCbXJycksWLCAoKAgoqOj\nlQVWPD09mTJlCvfv32ft2rWYmppSv359fHx88Pf35+jRo4SFhbF06VLWrl2bIc7r16/j4+ODjY0N\n9evXJyIigs8//5xNmzaxd+9e1Go1rq6uDBo0KNt2KlmyJGXKlCE8PJzjx4/z3nvvUbt2bSBtPfp/\n//vfdOzYkbFjx7Jx40ZGjx6trMxXqlQpduzYkWGlvmvXrjF9+nRMTU1p2LAhkyZNwtPTk2nTplGz\nZk02btxIREQELi4urF27lri4ON59912AXLVPdgICAujTpw+HDh3izJkzNG3aNMffldyUpn38+DH/\n+c9/9N735ptvMnHiROXxyZMnlUp9zs7OPH/+nJiYGKyt00aTIyIisLW1VQoONW3alBMnTmBhYUGz\nZs2wtrbG2to631ehM6rEHpf4HHPzjAvw/xMWFha0bdsWrVabaeUeIUTuFLayrQ0bNsTJyQlfX18S\nEhJwdXWlT58+yjrp7733HgsXLuTKlSvA/0qq7tq1C41Gw8aNG3n8+DFeXl4cPHhQ77hhYWGMGjWK\npk2bsn37djZv3syAAQMYM2YMo0ePJjIykqdPn1KpUiV8fX3ZunUrGo2GsWPHcu7cOQAePXrEli1b\nSEpKyjTOlStXMmrUKDp06MDYsWOxtLTkwYMHBAYG8uOPPwLg4eFBp06dqFChQpZtdPv2bZ4+fUrZ\nsmW5ffs2b731lt7rJUqUoHTp0oSFhXH79m0l6ad7OakDzJkzhylTptCwYUMmTpxIaGholse/efMm\nBw8e5OnTp7lun6xWZkxNTeXAgQP8+OOPWFhYsH///hwTe25L05YtW5YNGzZku68nT57oLZ3r4OBA\neHi4ktgdHByIjY3l7t27ODk5cfr0aVxc0goaJSQkMGLECKKiohgzZoxSvS8/GFVi15E3ifePP/7g\nwIED+Pr6Amm/pOlnckII45Fd2VZzc3OeP3+Ou7s7ZmZmREREAGm9zfRrs+m9sR9//FGvhGt677Js\n2bJoNBoiIyMpWbKkctwyZcowZ84c/Pz8iIqKol69epQvXx6VSkVYWBgnTpzA1dWVW7du8fDhQwYP\nHgykJZmHDx8Cab1BlUqVZZzBwcHKynPt27fn5MmTXL58mXv37uHl5QWkLcMaGhqaIbH/8MMPHDx4\nkJiYGJKSkli0aBEajQaVSpVpOdP0ynKZlXjNzJ07d6hZsyYACxcuzHbbWrVqodFoXql9skrsZ86c\noUKFClSoUIH333+fVatWMW3atBzjzY8SrpCxjKtKpeLzzz9n8uTJ2NjYULFiReW1yMhIVqxYwcOH\nD/Hy8iIoKCjf8o5RJfa8uN1Np9Ph4+PDmTNn6N69O/Xq1ZOkLkQeKGxlW8+cOcOpU6fYsGEDZmZm\nNGzYEAATE5NMR+fS6z+A/hd2UlISSUlJSv3wwYMHc/DgQVq2bImHhweBgYFKJTVXV1eOHDnC8ePH\nGT58OCqVivr162coDerv768cL6s406ungX4J17Zt2yplY7OSfo09LCyMAQMGKJPE0ku4fvDBB8q2\nsbGxPH/+nDJlylC9enUuXbqkt1ZHcHAw5cqV01sXPqfr2umV4kB/7fPctk9WAgICCA0NVeKPj4/n\nxIkTvPPOO8TExOht++zZMxwdHXNdmjY3Q/GOjo48efJEeRwWFkaZMmX03uPi4sLmzZsBWLx4MU5O\nTiQkJNCwYUNMTU2pXLkyVlZWPHv2jFKlSuXqc78q41qg5h8k9vQyjSqVimXLlrFz584sqxEJIYxf\nREQE5cqVw8zMjMOHD5OSkkJSUhL169fn1KlTACxbtowTJ07ove/FEq6PHj1CrVbj6OiolPVs27at\nUnZVp9Nx+PBhvRKuv/76K/fu3aNevXpUq1aN4OBgnj59CsDy5ct5/PhxruKsXLmycpng6NGjANSr\nV4/Tp08THx+PTqdjzpw5JCQkZNkGjo6O9OjRgxUrVgDQrVs3jhw5wuXLl5Vtli5dSu/evYG0E4IV\nK1Yo8YaHh/Ppp5/y6NEjvf06Ozsr+5g8eTLBwcFYW1sTHh4OwPnz5zON53XaJ11SUhJBQUHs3r1b\n+Td9+nQCAgKwsrLCwcGB33//HUi7uykwMJDmzZsDuStNmz4U/+K/F5M6pJVwTb8sc/XqVRwdHZVh\n+HRDhgzh6dOnxMXFERQURLNmzWjZsiWnTp0iNTWViIgI4uLisLe3z/Rz5gWj6rG/bl5fvnw5y5Yt\n48iRI1SqVIlatWq91m0OQgjj0bx5c7777jv69++Pq6srbdu2ZebMmXh7e+Pr68vmzZspX748o0eP\nZs+ePcr7unTpwpkzZ/D09ESr1WbaO3Zzc2P27Nk4OTkpk8aOHz9Oy5YtefDggTLZztLSksmTJzN0\n6FA0Gg1169bF0VF/nlBWcY4cOZKpU6eyfv16atSoQXR0NBUqVMDLy4t+/fphYmKCq6trjgWoBg4c\nSLdu3ejVqxdvvPEG3377LTNnziQ2Npbk5GRatmzJ8OHDAZQSr4MHD8bS0hJTU1OmTJlCjRo19PY5\nZcoUpk+fjlqt5u2338bZ2Rk3Nzc+++wzqlSpotSaf1n16tVz1T7h4eH4+fnptf3Ro0dp3LixXkLs\n2LEjS5YsITExkYULFzJ79myWLVuGVqtl4MCByvd8bkrT5kajRo2oV68e7u7uqFQqZsyYAaSNwNjY\n2NChQwf69u3LoEGDUKlUDBs2TJlI17FjR/r27QvA1KlTczWb/3UZVdnW6jWrYG9TOuc3vGTz5s3M\nnz+f77//XqlBLDIqCuVEjUFRaGcp25r//vjjDywsLKhduzbffPMNOp2OESNGGDosRX638YIFC5g0\naVK+7b+wycuyrUbVYzczyd2XiFar5YcffsDLywszMzM8PDzo3r17hiETIYQorDQaDVOmTMHCwgIL\nCwsWL15s6JAKTFJSEi1atDB0GEbLyBJ77s5YFi9ezMKFC4mLi2PMmDGoVCpJ6kIIo1K3bl127Nhh\n6DAMQqPRZFg7QOSeUSX27KSmpirXLEaOHElMTAwDBgwwcFRCCCFEzl68C+KfMqpZ8Ukp8Zk+f/36\ndTp06KDMHLWzs2POnDmZLqwghPjn1Gq13i1NQoh/JiUlJc8m1BlVjz0li4UT4uLiuHTpEr/++iut\nW7cu4KiEKH5MTU2Jj48nLi4OExOTQrcWhFar1buVSeQ9aeO8odPpSElJISUlJc/qkxhVj/3Fr47r\n168TFhYGpN2CcPLkyVytQCSEyBs2NjbKamaFTXBwsKFDKPKkjfOGSqVCo9FgY2OTZ/vM1x57duXt\nTpw4wZIlSzAxMaF169aMGjUqF3tM+wL5448/6NSpE//617/44YcfADLcZymEyH+FuQJiYb0VryiR\nNi6c8q3H/mJ5u7lz5zJ37ly919PXWf7xxx/57bffuHXrVo77TO8YNGjQgC5duvDRRx/lR+hCCCGE\n0cq3xJ5VeTuABw8eYGdnR/ny5VGr1bRp04aTJ0/muM/tP6Xd+qFWq1mzZg2dOnXKr/CFEEIIo5Rv\n42jZlbcLDw9XltlLf+3BgwdZ7it9cbx9AQfo29tdr1iDyFuJiYmGDqFYkHbOf9LG+U/aOH+lT058\n1QViC+wC2T9ZuTa9wMJk38ncuHEjr0ISmUgvOiHyl7Rz/pM2zn/SxgVDq9XmWBPgRfmW2LMrb/fy\na48fP85QGOFFVlZW1KxZEzMzs0I5A1cIIYTIazqdDq1W+8pr8udbYm/RogV+fn64u7tnKG9XsWJF\nYmJiCAkJoVy5cgQFBbFo0aIs96VWq/P0VgAhhBDCGLxKTz1dvlZ3W7RoEb///rtS3u7atWtKabuz\nZ88qyfxf//oXgwcPzq8whBBCiGLDKMq2CiGEECJ3jGrlOSGEEEJkTxK7EEIIUYQUysQ+b9483Nzc\ncHd359KlS3qvnThxgt69e+Pm5sbKlSsNFKHxy66NT506Rd++fXF3d8fX15fULIrviOxl18bpFi9e\njKenZwFHVnRk18aPHj3Cw8OD3r17M336dANFWDRk186bNm3Czc0NDw+PDCuMity7efMmrq6ubNy4\nMcNrr5z3dIXM6dOndcOGDdPpdDrdrVu3dH379tV7/f3339c9fPhQl5KSovPw8ND99ddfhgjTqOXU\nxh06dNA9evRIp9PpdGPGjNEdOXKkwGM0djm1sU6n0/311186Nzc3Xf/+/Qs6vCIhpzb29vbW/fe/\n/9XpdDrdzJkzdaGhoQUeY1GQXTtHR0fr2rVrp9NqtTqdTqcbOHCg7sKFCwaJ05jFxsbq+vfvr5s6\ndapuw4YNGV5/1bxX6Hrs+bEUrdCXXRsD+Pv7U65cOSBtVcCIiAiDxGnMcmpjgM8//5xx48YZIrwi\nIbs2Tk1N5dy5c7Rv3x6AGTNmUKFCBYPFasyya2czMzPMzMyIi4sjOTmZ+Ph47OzsDBmuUdJoNHz3\n3XeZrufyOnmv0CX2J0+eYG9vrzxOX4oWyHQp2vTXRO5l18aAst5AWFgYv/32G23atCnwGI1dTm3s\n7++Pi4sLTk5OhgivSMiujZ89e4aVlRXz58/Hw8ODxYsXGypMo5ddO5ubmzNq1ChcXV1p164db731\nFtWqVTNUqEbL1NQ0y/vVXyfvFbrE/jKd3I2X7zJr46dPnzJixAhmzJih90ctXs+LbRwZGYm/vz8D\nBw40YERFz4ttrNPpePz4MV5eXmzcuJFr165x5MgRwwVXhLzYzjExMXzzzTcEBgZy+PBhLl68yPXr\n1w0YnYBCmNjzcilakbns2hjS/liHDh3Kp59+SsuWLQ0RotHLro1PnTrFs2fP6NevH6NHj+bq1avM\nmzfPUKEareza2N7engoVKlC5cmVMTExo1qwZf/31l6FCNWrZtXNwcDCVKlXCwcEBjUbDO++8I+vH\n57HXyXuFLrG3aNGCgwcPAmS7FG1ycjJBQUG0aNHCkOEapezaGNKu/Q4YMIDWrVsbKkSjl10bd+rU\nif3797Nt2zZWrFhBvXr1mDx5siHDNUrZtbGpqSmVKlXi7t27yusyRPx6smtnJycngoODSUhIANKK\nwlStWtVQoRZJr5P3CuXKc7IUbf7Lqo1btmxJkyZNaNiwobJt165dcXNzM2C0xim73+N0ISEh+Pr6\nsmHDBgNGaryya+N79+7h4+ODTqejZs2azJw5E7W60PVljEJ27bxlyxb8/f0xMTGhYcOGTJw40dDh\nGp0rV64La7bZAAAGX0lEQVSwYMECQkNDMTU1pWzZsrRv356KFSu+Vt4rlIldCCGEEK9HTl+FEEKI\nIkQSuxBCCFGESGIXQgghihBJ7EIIIUQRIoldCCGEKEJMDR2AEMVBSEgInTp10ruNEGDy5MnUqVMn\n0/f4+fmRnJz8j9aTP336NJ988gl169YFIDExkbp16zJlyhTMzMxeaV9Hjx7l6tWrjBw5kvPnz1Om\nTBkqVarE3Llz+eCDD6hfv/5rx+nn54e/vz8VK1YEIDk5mXLlyvHZZ59hY2OT5fseP37M7du3adas\n2WsfW4iiRhK7EAXEwcHBIPer16xZUzmuTqdj3LhxbN26lf79+7/Sflq3bq0sWuTv70/nzp2pVKkS\nU6ZMyZM4u3fvrncS88UXX/D1118zYcKELN9z+vRpgoODJbEL8QJJ7EIYWHBwMDNmzMDExISYmBg+\n/fRTWrVqpbyenJzM1KlTuXPnDiqVijp16jBjxgySkpL47LPPuHfvHrGxsXTt2pVBgwZleyyVSkXj\nxo25ffs2AEeOHGHlypVYWFhgaWnJ7NmzKVu2LIsWLeLUqVNoNBrKli3LggULCAgI4MSJE3Ts2JHA\nwEAuXbqEr68vX331FSNHjmTx4sVMmTKFRo0aAfDxxx8zcOBA3njjDWbNmkV8fDxxcXH8+9//pnnz\n5jm2S8OGDdm2bRsAv//+O4sWLUKj0ZCQkMCMGTOwtbXlyy+/RKfTUbJkSfr16/fK7SFEUSSJXQgD\ne/LkCWPHjqVJkyZcuHCB2bNn6yX2mzdvcvHiRQ4cOADAtm3biI6OZuvWrTg6OjJnzhxSUlLo27cv\nzZs3p3bt2lkeKzExkaCgIHr37k18fDxTp05l+/btlCtXjo0bN/Lll1/i4+PDpk2b+P333zExMWH/\n/v16a1V36NCBH374gZEjR9KsWTO++uorALp168bBgwdp1KgRT58+JTg4mJYtWzJy5EgGDRpE06ZN\nCQ8Px83Njf/+97+Ymmb99ZOcnExAQABvv/02kFY4Z+bMmdSuXZuAgAC++eYbli9fTs+ePUlOTmbg\nwIGsXr36ldtDiKJIErsQBeTZs2d4enrqPbds2TLKlCnDwoULWbp0KVqtlsjISL1tnJ2dsbe3Z+jQ\nobRr1473338fGxsbTp8+zd9//83Zs2cBSEpK4v79+xkS2c2bN/WO265dOzp37syff/5JqVKlKFeu\nHAAuLi5s2bIFOzs7WrVqRf/+/enQoQOdO3dWtslOly5d8PDwwNfXl8DAQDp16oSJiQmnT58mNjaW\nlStXAmnruD99+pSyZcvqvX/Pnj2cP38enU7HtWvX8PLyYtiwYQCULl2ahQsXkpiYSHR0dKY1v3Pb\nHkIUdZLYhSggWV1jHz9+PF26dKF3797cvHmTESNG6L1ubm7O5s2buXr1qtLb/vHHH9FoNIwaNYpO\nnTple9wXr7G/SKVS6T3W6XTKc8uXLyc4OJhff/2V/v374+fnl+PnS59Md+nSJQ4cOICPjw8AGo0G\nPz8/vZrSmXnxGvuIESNwcnJSevUTJ05k1qxZNGvWjKCgINauXZvh/bltDyGKOrndTQgDe/LkCW+8\n8QYA+/fvJykpSe/1y5cvs3PnTurVq8fo0aOpV68ed+/epXHjxsrwfGpqKvPnz8/Q289O1apVefr0\nKQ8fPgTg5MmTvPXWWzx48IB169bh7OzMoEGD6NChQ4Ya2yqVCq1Wm2Gf3bp1Y/v27Tx//lyZJf9i\nnM+ePWPu3Lk5xjZjxgz8/Pz4+++/9dooJSWFwMBApY1UKhXJyckZjvM67SFEUSGJXQgDGzRoEBMn\nTmTw4ME0btwYOzs7Pv/8c+X1ypUrc/DgQdzd3fHy8sLW1pZGjRrRr1+//2vvjlEdhOE4jv9GEQS3\n4j0cBPcsnQRRECRTxFHnIk7dPUJP0MWtdPcG3kC8RrcHnd6Dt4XvZ/xDSMjyJVMUhqHqulZVVYqi\nSHEc/3nfIAh0v981jqPattW2bRqGQZfLRfu+qyxLWWt1HIeMMV9r8zzXPM96vV5fc2OM1nXV9Xr9\nmd1uN73fbzVNo67rlGXZr2dLkkTOOU3TJElyzslaq77vVRSFzvPU4/FQmqZ6Pp9aluXf9wH4gt/d\nAADwCC92AAA8QtgBAPAIYQcAwCOEHQAAjxB2AAA8QtgBAPAIYQcAwCOEHQAAj3wASvoPYTunjfgA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wXnM5YYecEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "402cc29e-6d8a-4bd8-d760-d3c8a2ef87c0"
      },
      "source": [
        "clf = DecisionTreeClassifier(random_state=0,max_depth=30,min_samples_leaf=20)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "\n",
        "dt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "dt_lm.fit(X_test, y_test)\n",
        "\n",
        "y_pred_dt = clf.predict_proba(X_test)[:, 1]\n",
        "roc = roc_curve(y_test, y_pred_dt)\n",
        "roc_auc = auc(fpr_dt, tpr_dt)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_dt, tpr_dt, label='ROC of DT (AUC = %0.2f)' % roc_auc)\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8d9287681ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    316\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    317\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
          ]
        }
      ]
    }
  ]
}